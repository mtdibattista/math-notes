\documentclass[12pt, twoside, draft]{article}

\usepackage{graphicx}
\usepackage{amsmath, amsthm, amsfonts}
\usepackage[numbers]{natbib}
\usepackage{algorithm, algorithmic}
\usepackage{placeins}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\usepackage{xfrac}
\usepackage{relsize}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{mathtools}
\setcounter{secnumdepth}{5}
\usepackage{lineno}
\usepackage{bm}

\usepackage[text={16cm,23cm},centering]{geometry}

\setlength{\parskip}{1.2ex}
\setlength{\parindent}{0em}
\clubpenalty = 100
\widowpenalty = 100
\renewcommand{\baselinestretch}{1.0}

\newcommand*{\red}{\textcolor{red}}
\newcommand*{\green}{\textcolor{green}}
\newcommand*{\blue}{\textcolor{blue}}

\begin{document}

\setpagewiselinenumbers

%\modulolinenumbers[5]
%\linenumbers

\title{Linear Algebra (Applications)}

\footnotesize\date{\today}

\author{Mark DiBattista}
%\\
%\footnotesize \texttt{mtdibattista@gmail.com} \\ }

\maketitle

\begin{abstract}
The mathematical analysis of physical objects in multidimensional settings is best handled using the tools of Linear Algebra.  Modeling objects as vectors and linear (or linearized) interactions as matrices, we can gain insight into linear equations and solutions through geometric interpretations provided by eigenstructure decompositions or algebraic properties of concatenated operators.  Here, we show simple applications of linear algebraic tools to basic problems in geometry, calculus and estimation. 
\end{abstract}
%{\bf Keywords:} enter, keyword, here.

%\tableofcontents

\section{Suggested Resource Materials}
Useful source texts:

\begin{itemize}[noitemsep]
\item Basic linear algebra:\hspace{50pt} \textit{Applied Linear Algebra}, Noble \& Daniel
\item[] \hspace{140pt} \textit{Introduction to Linear Algebra}, Strang
\item Computational perspective: \hspace{13pt} \textit{Matrix Computations}, Golub
\item Optimization applications: \hspace{18pt} \textit{Practical Optimization}, Gill, Murray \& Wright
\item Many, many matrix formulas: (http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)
\end{itemize}

Throughout the text the acronym, \textit{LAN}, refers to the companion writeup, \textit{Linear Algebra (Notes)}, in which information is referenced by chapter and/or numbered equation.

\section{Real Functions and Vector Operators}\label{sec:real_functions}

\subsection{Conic Functions}

Two-dimensional scalar conic functions can be expressed as the level curves of the quadratic form (\textit{cf. LAN}, $\S$3.8),

\begin{equation}\label{eq:conic_section}
f(\mathbf{x}) = \mathbf{x}^\top A \mathbf{x}, 
\end{equation}

for which the matrix, $A$, is self-adjoint: $A = A^\top$.  The classification of conic functions -- ellipse, hyperbola or parabola -- corresponds to the signs of the eigenvalues of the matrix, $A$.

 The eigensystems of self-adjoint matrices are given by the Spectral Theorem (\textit{cf. LAN}, $\S$4.2): all eigenvalues are real, and all associated eigenvectors form an orthonormal set.  Diagonalizing the matrix we have

\begin{equation}
A = VDV^\top \Rightarrow \begin{cases} \text{eigenvalues: } D \text{ is diagonal, } D =\begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix} \\
\text{eigenvectors: } V  \text{ is orthogonal, } VV^\top = V^\top V = I \end{cases}
\end{equation}

Introducing a change of basis (\textit{cf. LAN}, $\S$3.9.3) to align with coordinate system that coincides with the eigenvectors: 

\begin{equation}
\mathbf{x}' = V^\top \mathbf{x} \Rightarrow \begin{cases} \mathbf{x}^\top A \mathbf{x} &= \mathbf{x}^\top VDV^\top \mathbf{x} \\
&= \left(V^\top \mathbf{x} \right)^\top D \left(V^\top \mathbf{x} \right) \\ &= \mathbf{x}'^\top D \mathbf{x}' \end{cases}
\end{equation}

the conic equations take the form,

\begin{equation}\label{eq:eigenvalue_conic_conditions}
f(x', y') = \lambda_1 x'^2 + \lambda_2 y'^2 \Rightarrow \begin{cases} \lambda_1, \lambda_2 \text{ same sign,}  &\text{ ellipse} \\ 
\lambda_1, \lambda_2 \text{ different sign,} &\text{ hyperbola} \\
\lambda_1 \text{ or } \lambda_2 = 0, &\text{ parabola} \end{cases}
\end{equation}

In general if the matrix, $A$, is an $n \times n$ positive- or negative-definite matrix, then the level surfaces defined by the equation in (\ref{eq:conic_section}) take the form of nested ellipsoids.

\subsubsection{Link between the Discriminant and the Characteristic Equation}\label{sec:discriminant_characteristic_equation}
Filling in the entries in the 2-dimensional matrix, $A$, leads to the standard conic equation in basic algebra:

\begin{equation}\label{eq:conic_equation}
\begin{rcases} A= \begin{pmatrix} a & \frac{1}{2}b \\ \frac{1}{2}b & c \end{pmatrix} \\ 
\mathbf{x} = \begin{pmatrix} x \\ y \end{pmatrix} \end{rcases} \Rightarrow 
\mathbf{x}^\top A \mathbf{x} = ax^2 + bxy + cy^2.
\end{equation}

The characteristic equation (\textit{cf. LAN}, $\S$4, (83)) of the matrix,

\begin{equation}
\det(A - \lambda I) = \begin{vmatrix} a - \lambda & \frac{1}{2}b \\ \frac{1}{2}b & c - \lambda \end{vmatrix} = \lambda^2 - (a+c) \lambda - \frac{1}{4}(b^2 - 4ac) = 0,
\end{equation}

has the solution

\begin{equation}
\lambda = \frac{(a+c) \pm \sqrt{(a+c)^2 + (b^2 - 4ac)}}{2} = \frac{a+c}{2} \left( 1 \pm \sqrt{1 + \frac{b^2 - 4ac}{(a+c)^2}} \right).
\end{equation}

The relative signs of the eigenvalues are determined by the sign of the \textbf{discriminant},

\begin{equation}
b^2-4ac \Rightarrow \begin{cases}
 > 0, &\text{  ellipse} \\
 = 0, &\text{  parabola} \\
< 0, &\text{  hyperbola} \end{cases}
\end{equation}

which match the conditions (\ref{eq:eigenvalue_conic_conditions}).  Also, note that the characteristic equation can be expressed in terms of the trace and determinant of the matrix:

\begin{equation}
\lambda = \frac{\operatorname{tr}A}{2} \left( 1 \pm \sqrt{1 + \frac{\det A}{\operatorname{tr}^2 A}} \right),
\end{equation}

both of which are invariant under orthogonal transformation of the matrix.

\subsection{Differential Vector Operators}\label{sec:differential_vector_operators}
\subsubsection{Gradient and Hessian}\label{sec:gradient_hessian}
Multidimensional functions, $f: \mathbb{R}^n \rightarrow \mathbb{R}$, map vectors in the $n$-dimensional domain to a scalar function.  The directional derivatives of multidimensional functions, for which the derivatives are taken in directions that coincide with the natural basis of the vector space, can be ordered in a vector form that contains the complete information necessary to calculate the rate of change of the function in arbitrary directions of the space.  The \textbf{gradient operator} for the function, $f$, takes the form

\begin{equation}
\text{Gradient: }\nabla \equiv \begin{pmatrix} \frac{\partial}{\partial x_1} \\ \vdots \\ \frac{\partial}{\partial x_n} \end{pmatrix} \Rightarrow
\nabla f(\mathbf{x}) \equiv \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix},
\end{equation}

 and the directional derivative at the arbitrary point, $\mathbf{x}$,  in the arbitrary direction, $\mathbf{s}$, is expressed as the inner product,

\begin{equation}
\langle \mathbf{s}, \nabla f(\mathbf{x}) \rangle \equiv \mathbf{s}^\top \nabla f(\mathbf{x}) \equiv \nabla f(\mathbf{x})^\top \mathbf{s} \equiv \sum_{i=1}^n s_i \frac{\partial f}{\partial x_i}.
\end{equation}

Similarly the \textbf{Hessian operator} for the function, $f$, records the second-order directional derivatives, again in directions aligned with the natural basis vectors, taken pairwise in matrix form ,

\begin{equation}\label{eq:Hessian}
\text{Hessian: }H \equiv 
\begin{pmatrix} 
\frac{\partial^2}{\partial x_1^2} & \cdots & \frac{\partial^2}{\partial x_1 \partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial^2}{\partial x_m \partial x_1} & \cdots & \frac{\partial^2}{\partial x_m \partial x_n}
\end{pmatrix} \Rightarrow
H f(\mathbf{x}) \equiv 
\begin{pmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_m \partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_m \partial x_n}
\end{pmatrix}.
\end{equation} 

The curvature of the surface at the arbitrary point, $\mathbf{x}$, in the arbitrary direction, $\mathbf{s}$, is given by the quadratic form,

\begin{equation}
\langle \mathbf{s}, H f(\mathbf{x})  \mathbf{s} \rangle \equiv \mathbf{s}^\top H f(\mathbf{x})  \mathbf{s}.
\end{equation}

In the special cases for which the function, $f$, is itself an inner product or quadratic form the gradient is a function of the underlying vector,

\begin{align}\label{eq:gradient_operations}
&\text{inner product : } f(\mathbf{x}) = \langle \mathbf{x}, \mathbf{c} \rangle \Rightarrow \nabla_\mathbf{x} \left( \mathbf{x}^\top \mathbf{c} \right) = \nabla_\mathbf{x} \left( \mathbf{c}^\top \mathbf{x} \right) = \mathbf {c}; \\
&\text{quadratic form: } f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top C \mathbf{x} \Rightarrow  \nabla_\mathbf{x} \left( \frac{1}{2} \mathbf{x}^\top C \mathbf{x} \right) = C \mathbf{x}.
\end{align}

Here, the subscripts indicate the variables against which the derivatives are applied.

\subsubsection{Taylor's Approximations in Multidimensional Spaces}\label{taylors_theorem}
Taylor's Theorem links information on a function at a single point -- the value of the function and derivatives at all orders at the point -- to the value of the function at arbitrary points at arbitrary distances.  If we retain only partial information locally -- only the value at the point and the first and second derivatives -- we obtain an approximation for the value of the function at arbitrary points, an approximation that degrades with the distance of the arbitrary point.

Choosing the local point as $\mathbf{x}$, the second-order \textbf{Taylor's approximation for scalar functions}, $f: \mathbb{R}^n \rightarrow \mathbb{R}$, evaluated at the arbitrary point, $\mathbf{x} + \Delta \mathbf{x}$,  depends only on the gradient and Hessian of the function,

\begin{equation}\label{eq:Taylors_theorem_scalar}
f(\mathbf{x} + \Delta \mathbf{x}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \Delta \mathbf{x} + \frac{1}{2} \Delta \mathbf{x}^\top H f(\mathbf{x}) \Delta \mathbf{x}.
\end{equation}

Furthermore, \textbf{Taylor's Theorem for multidimensional functions}, $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ leads to the vector representation for which the gradient and Hessian operators are $n$-dimensional, and the result is stored in the $m$-dimensional range space,

\begin{equation}\label{eq:Taylors_theorem_vector}
\mathbf{f}(\mathbf{x} + \Delta \mathbf{x}) \equiv 
\begin{pmatrix}
f_1(\mathbf{x} + \Delta \mathbf{x}) \\
\vdots \\
f_m(\mathbf{x} + \Delta \mathbf{x})
\end{pmatrix}
\approx 
\begin{pmatrix}
f_1(\mathbf{x}) + \nabla f_1(\mathbf{x})^\top \Delta \mathbf{x} + \frac{1}{2} \Delta \mathbf{x}^\top Hf_1 \Delta \mathbf{x} \\
\vdots \\
f_m(\mathbf{x}) + \nabla f_m(\mathbf{x})^\top \Delta \mathbf{x} + \frac{1}{2} \Delta \mathbf{x}^\top Hf_m \Delta \mathbf{x}
\end{pmatrix}.
\end{equation}

The equations in (\ref{eq:Taylors_theorem_scalar}) and (\ref{eq:Taylors_theorem_vector}) can be interpreted as second-order algebraic approximations to the true function, $f$.  Frequently, this amounts to replacing the complicated surface with elliptic paraboloids or hyperboloids whose value, gradient and Hessian match the function at the point, $\mathbf{x}$.  Numerical methods in optimization generate extrema in non-linear functions, $f$, by calculating the extrema of a series second-order approximations, and iterating until convergence.

\subsubsection{Jacobian and Differential Volume}
The volume of an $n$-dimensional parallelepiped whose vertex is enclosed by the vectors, $\mathbf{c}_1, \ldots, \mathbf{c}_n$, is given by, (\textit{cf. LAN}, $\S$3.7, (56)),

\begin{equation}
A = \begin{pmatrix} \mathbf{c}_1 & \cdots & \mathbf{c}_n \end{pmatrix} \Rightarrow \text{volume: } |\det A| = \left| \bigwedge_{i=1}^n \mathbf{c}_i \right|.
\end{equation}

The Cartesian \textbf{differential volume element} for an $n$-dimensional space can be represented as the wedge product of the differentials associated with each direction,

\begin{equation}
dV = dx_1 \wedge \cdots \wedge dx_n.
\end{equation} 

If we introduce a change of coordinate system, $(x_1, \ldots, x_n)  \rightarrow (x'_1, \ldots, x'_n)$, then the \textbf{Jacobian matrix},

\begin{equation}\label{eq:Jacobian}
J = 
\begin{pmatrix}
\frac{\partial x_1}{\partial x'_1} & \cdots & \frac{\partial x_1}{\partial x'_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial x_m}{\partial x'_1} & \cdots & \frac{\partial x_m}{\partial x'_n}
\end{pmatrix},
\end{equation}

gives the `volume element' of the transformation.  From the transformation of (total) differentials we can construct the transformed volume element,

\begin{equation}\label{eq:total_differential}
dx_i = \sum_{j=1}^n \frac{\partial x_i}{\partial x'_j} dx'_j \Rightarrow \bigwedge_{i=1}^n dx_i = |J| \bigwedge_{j=1}^n dx'_j \Rightarrow dV = |J| dV',
\end{equation}

whose scaling is given by the determinant of the Jacobian matrix.

Notica also that the Hessian matrix in (\ref{eq:Hessian}) can be expressed as the Jacobian of the gradient of the transformation.

\section{Least Squares}\label{sec:least_squares}
Given an $m \times n$ data matrix, $A$, and an $m \times 1$ vector, $\mathbf{b}$, it is a common optimization problem to find the $n \times 1$ vector, $\mathbf{x}$ that minimizes the distance between the linear transformation, $A\mathbf{x}$, and the target vector, $\mathbf{b}$.  For the case in which $m = n$, and the matrix is square, the possible solutions to the equation,

\begin{equation}
A \mathbf{x} - \mathbf{b} = 0,
\end{equation}

are governed by the Fredholm Alternative (\textit{cf. LAN}, $\S$7.1), with no solution, a unique solution, or an infinity of solutions depending on the rank and the relative extent of the image of the matrix.  For the \textbf{overdetermined problem}, $m > n$, the probability that the target vector, $\mathbf{b}$, lies in the column space of $A$ is remote, and optimization determines the vector, $\mathbf{x}$, that minimizes the distance between transformed and target vectors.  If the `distance' is interpreted as the vector 2-norm, we have the \textbf{least-squares problem},

\begin{equation}\label{eq:least_squares}
\min_{\mathbf{x}} || A \mathbf{x} - \mathbf{b} ||_2^2 \Rightarrow \nabla_\mathbf{x} \left( A \mathbf{x} - \mathbf{b} \right)^\top\left( A \mathbf{x} - \mathbf{b} \right) = 0,
\end{equation}

which expresses the distance-minimizing solution as the vanishing gradient of an inner product.

\subsection{Generalized Inverse Solution}
Expanding the inner product in (\ref{eq:least_squares}) and applying the gradient operator, defined in (\ref{eq:gradient_operations}), term-by-term yields,
\begin{align}\label{eq:least_squares_details}
\nabla_\mathbf{x} \left( A \mathbf{x} - \mathbf{b} \right)^\top \left( A \mathbf{x} - \mathbf{b} \right) & =
\nabla_\mathbf{x} \left( A \mathbf{x} \right)^\top \left( A \mathbf{x} \right) - \nabla_\mathbf{x} \left( A \mathbf{x} \right)^\top \mathbf{b} - \nabla_\mathbf{x} \mathbf{b}^\top A \mathbf{x} + \nabla_\mathbf{x} \mathbf{b}^\top \mathbf{b} \\
& = \nabla_\mathbf{x} \mathbf{x}^\top A^\top A \mathbf{x} - \nabla_\mathbf{x} \mathbf{x}^\top A^\top \mathbf{b} - \nabla_\mathbf{x} \mathbf{b}^\top A \mathbf{x} + \nabla_\mathbf{x} \mathbf{b}^\top \mathbf{b} \\
& = 2 A^\top A \mathbf{x} - 2 A^\top \mathbf{b} = 0 \\
\Rightarrow \mathbf{x}_{LS} &= \left( A^\top A \right)^{-1} A^\top \mathbf{b}.
\end{align}

Here, the least-squares solution is achieved by applying the \textbf{generalized inverse} operator, $\left( A^\top A \right)^{-1} A^\top$, to the target vector, an operation that requires the data matrix to be of full rank, $\operatorname{rank} (A) = n$.  Note that the action of the matrix, $A$, on the least-square solution, $\mathbf{x}_{LS}$, is equivalent to projecting the target vector onto the column space of $A$,

\begin{equation}\label{eq:least_squares_projection}
A \mathbf{x}_{LS} = A \left( A^\top A \right)^{-1} A^\top \mathbf{b} \equiv P_A \mathbf{b}
\end{equation}

See the discussion in \textit{LAN}, $\S$3.10 for properties of projection operators.

\subsection{Iterative Solution (Bayesian Form)}
The solution to the least-squares problem in (\ref{eq:least_squares_details}) requires \textit{all} information in the data matrix to generate the generalized inverse.  If, for example, data is gathered at two separate time points, or if data points are arriving continuously, generalized inverses are created from ever-larger data matrices that require increasing numbers of computations.  Since the data points that make up the matrix are independent and all operations in the creation of the generalized inverse are linear, it is possible to derive an iterative solution that continually builds upon prior information, one that can be cast into Bayesian terms (cf. \textit{PN}, $\S$11).  The two methods generate identical solutions -- the iterative method is simply the more efficient implementation.

Let $A_i$ be an $m \times n$ matrix that represents $m$ prior data measurements (here, $i=m$) taken from an $n$-dimensional space with known inverse covariance matrix, $\left( A_i^\top A_i \right)^{-1}$, and let the vector, $\mathbf{b}_i$, be the associated prior target vector of responses.  A single additional datapoint, $\mathbf{a}_{i+1}$, and response, $b_{i+1}$, can be represented in block form as

\begin{align}
A_{i+1} = \begin{pmatrix} A_i \\ \mathbf{a}_{i+1}^\top \end{pmatrix} \\
\mathbf{b}_{i+1} = \begin{pmatrix} \mathbf{b}_i \\ b_{i+1} \end{pmatrix} .
\end{align}

The new inverse scatter matrix can also be represented in block form as

\begin{equation}\label{eq:updated_inverse_scatter_matrix}
\left( A_{i+1}^\top A_{i+1} \right)^{-1} = \left( A_i^\top A_i + \mathbf{a}_{i+1} \mathbf{a}_{i+1}^\top \right)^{-1},
\end{equation}

whose expansion can be derived from the the Woodbury formula, \textit{cf. LAN}, $\S$6.3, 

\begin{equation}\label{eq:woodbury_formula}
\left( A - BD^{-1}C \right)^{-1} = (I + A^{-1}B \left( D - CA^{-1}B \right)^{-1}C) A^{-1}.
\end{equation}

Indentifying the blocks in (\ref{eq:woodbury_formula}) with the elements in the updated  inverse scatter matrix in (\ref{eq:updated_inverse_scatter_matrix}),

\begin{align}
&A \rightarrow A_i^\top A_i \\
&B \rightarrow \mathbf{a}_{i+1} \\
&C \rightarrow \mathbf{a}_{i+1}^\top \\
&D \rightarrow -1
\end{align}

yields the expansion,

\begin{align}\label{eq:updated_inverse_scatter_matrix}
\left( A_{i+1}^\top A_{i+1} \right)^{-1} &= \left( I - \frac{\left( A_i^\top A_i \right)^{-1} \mathbf{a}_{i+1} \mathbf{a}_{i+1}^\top }{1 + \mathbf{a}_{i+1}^\top \left( A_i^\top A_i \right)^{-1} \mathbf{a}_{i+1}} \right)  \left( A_i^\top A_i \right)^{-1} \notag \\
&= \left( I - \mathbf{k}_{i+1} \mathbf{a}_{i+1}^\top \right) \left( A_i^\top A_i \right)^{-1},
\end{align}

in which the updated \textbf{gain matrix}, 

\begin{equation}
\mathbf{k}_i \equiv  \frac{\left( A_i^\top A_i \right)^{-1} \mathbf{a}_{i+1}}{1 + \mathbf{a}_{i+1}^\top \left( A_i^\top A_i \right)^{-1} \mathbf{a}_{i+1}}
\end{equation}

depends only of the prior inverse scatter matrix, $(A_i^\top A_i)^{-1}$, and the new data point, $\mathbf{a}_{i+1}$.  Introducing the expansion in (\ref{eq:updated_inverse_scatter_matrix}) into the least-squares solution in (\ref{eq:least_squares_details}) yields

\begin{align}
\mathbf{x}_{i+1} &= \left( A_{i+1}^\top A_{i+1} \right)^{-1} A_{i+1}^\top \mathbf{b}_{i+1} \notag \\
&=  \left( I - \mathbf{k}_i \mathbf{a}_{i+1}^\top \right) \left( A_i^\top A_i \right)^{-1} \begin{pmatrix} A_i^\top & \mathbf{a}_{i+1} \end{pmatrix}
\begin{pmatrix} \mathbf{b}_i \\ b_{i+1} \end{pmatrix} \notag \\
&=  \left( I - \mathbf{k}_i \mathbf{a}_{i+1}^\top \right) \left( A_i^\top A_i \right)^{-1} \begin{pmatrix} A_i^\top \mathbf{b}_i + \mathbf{a}_{i+1} b_{i+1} \end{pmatrix} \notag \\
&= \left(I - \mathbf{k}_i \mathbf{a}_{i+1}^\top \right) \mathbf{x}_i + \mathbf{k}_i b_{i+1}.
\end{align}

Here, the updated solution, $\mathbf{x}_{i+1}$, is Bayesian in form for which the gain matrix, $\mathbf{k}_i$, determines the weighting between the prior estimate, $\mathbf{x}_i$, and the new information contained in the measurements, $\mathbf{a}_{i+1}$ and $\mathbf{b}_{i+1}$.

\end{document}
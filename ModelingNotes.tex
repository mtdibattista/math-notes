\documentclass[12pt, twoside, draft]{article}

\usepackage{graphicx}
\usepackage{amsmath, amsthm, amsfonts}
\usepackage[numbers]{natbib}
\usepackage{algorithm, algorithmic}
\usepackage{placeins}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\usepackage{xfrac}
\usepackage{relsize}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{mathtools}
\setcounter{secnumdepth}{5}
\usepackage{lineno}
\usepackage{bm}

\usepackage[text={16cm,23cm},centering]{geometry}

\setlength{\parskip}{1.2ex}
\setlength{\parindent}{0em}
\clubpenalty = 100
\widowpenalty = 100
\renewcommand{\baselinestretch}{1.0}

\newcommand*{\red}{\textcolor{red}}
\newcommand*{\green}{\textcolor{green}}
\newcommand*{\blue}{\textcolor{blue}}

\begin{document}

\setpagewiselinenumbers

%\modulolinenumbers[5]
%\linenumbers

\title{Modeling (Notes)}

\footnotesize\date{\today}

\author{Mark DiBattista}
%\\
%\footnotesize \texttt{mtdibattista@gmail.com} \\ }

\maketitle

\begin{abstract}
There are two parts to the modeling problem:
\begin{itemize}[noitemsep]
\item Given a complex phenomenon or process, generate a finite set of structured datapoints, sampled at random or by design, that covers its range of properties or behavior;
\item Given a finite set of structured datapoints, extract information that, given a new, partially complete datapoint, affords imputation of missing values from those present.
\end{itemize}
Usually, the information is incomplete and contradictory, and imputation provisional.

Models, and modeling techniques, are distinguished by the type of datapoints, the relationship among datapoints within a single sample, and linkage of datapoints across samples.  The relationships within datapoints may be constrained in form, while linkage across samples maybe  facilitated by sequencing or by mapping samples to an increasing variable such as time. 

The focus of these notes is on the quantitative aspect of the modeling problem for which uncertainties in the approximating relationships and linkages are sufficiently regular to support the expression and interpretation of imputed values as estimates and confidence intervals. 


\end{abstract}
%{\bf Keywords:} enter, keyword, here.

%\tableofcontents

\section{Suggested Resource Materials}
Useful source texts:

\begin{itemize}[noitemsep]
\item Probability/Statistics, intermediate (probability sections are better than statistics):\hspace{50pt}
\item[] \hspace{200pt} \textit{Statistical Inference}, Casella \& Berger
\item Probability, advanced: \hspace{97pt} \textit{Probability and Measure}, Billingsley
\item Computational Issues: Linear Algebra \hspace{29pt} \textit{Matrix Computations}, Golub \& Van Loan
\item[] \hspace{94pt}: Collinearity \hspace{43pt} \textit{Conditioning Diagnostics}, Belsley
\item Generalized Linear Models: \hspace{74pt} \textit{Multivariate Statistical Modelling Based on GLMs}
\item[] \hspace{345pt} Fahrmeir \& Tutz
\end{itemize}

Throughout the text the acronyms refer to companion writeups,
\begin{align}
&\textit{LAN} \hspace{10pt} \textit{Linear Algebra (Notes)} \notag \\
&\textit{LAA} \hspace{10pt} \textit{Linear Algebra (Applications)} \notag \\
&\textit{PN} \hspace{17pt} \textit{Probability Notes} \notag \\
&\textit{SN} \hspace{18pt} \textit{Statistics Notes} \notag
\end{align}
within which information is referenced by chapter and/or numbered equation.

\section{Linear Models}\label{sec:linear_models}
The linear model covers the case for which the data points are real values, partitioned into a single response variable and remainder predictor variables, and the true relationship between predictors and response is a linear function.  For any real problem the relationship encoded in the data points is noisy, however, and the assumption of linearity under common conditions for optimality is consistent with joint Gaussian distributions of response and predictor variables. 

In fact given a set of measurements, there are two basic approaches commonly taken to solve the problem.

\begin{itemize}
\item The \textit{engineering approach:} generate model coefficients for a prior linear relation between predictors and response variables that minimizes a loss function;
\item The \textit{probabilistic approach:} generate model parameters for a prior Gaussian conditional probability distribution, response variable conditioned on the predictors, that maximize an entropy measure.
\end{itemize}

The model parameters for which the loss function in the engineering approach is quadratic, also known as `least squares', and for which the entropy measure is maximum likelihood are identical, and each can be shown to generate sample estimators for the mean vector and covariance matrix of the underlying joint Gaussian distribution.  Finally, the estimated parameters of the Gaussian distribution can be used to generate interval tests and other measures of quality and stability of the model.

\subsection{Gaussian Joint and Conditional Probability Function}
A multivariate Gaussian distribution is completely defined by the specification of a mean vector, $\boldsymbol{\mu}$, and covariance matrix, $\boldsymbol{\Sigma}$, with the probability density function
\begin{equation}
\mathbf{W} \sim \operatorname{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}) \Rightarrow p_N(\mathbf{w}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{\sqrt{(2\pi)^n \det \boldsymbol{\Sigma}}} e^{-\frac{1}{2}(\mathbf{w} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{w} - \boldsymbol{\mu})}.
\end{equation}
The joint distribution can be expressed as the product of marginal and conditional distributions (\textit{cf. PN}, $\S$7.2.1.6) upon partitioning the variables into two distinct sets,
\begin{equation}\label{eq:partitioned_Gaussian}
\mathbf{W} = \begin{pmatrix} \mathbf{W}_1 \\ \mathbf{W}_2 \end{pmatrix} \Rightarrow
\begin{cases}
\boldsymbol{\mu} = \begin{pmatrix} \boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{pmatrix} \\ \\
\boldsymbol{\Sigma} = \begin{pmatrix} \boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12} \\ \boldsymbol{\Sigma}_{21} & \boldsymbol{\Sigma}_{22}, \end{pmatrix}
\end{cases}
\end{equation}
and the distribution of $\mathbf{W}_1$ conditioned on the realized variables, $\mathbf{W}_2 = \mathbf{w}_2$ is given by
\begin{equation}\label{eq:conditioned_Gaussian}
\mathbf{W}_1 | (\mathbf{W}_2 = \mathbf{w}_2) \sim \operatorname{N}(\boldsymbol{\mu}_1 - (\mathbf{w}_2 - \boldsymbol{\mu}_2)^\top \boldsymbol{\Sigma}_{22}^{-1} \boldsymbol{\Sigma}_{21}, \boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12} \boldsymbol{\Sigma}_{22}^{-1} \boldsymbol{\Sigma}_{21}).
\end{equation}

If the vector of random variables is partitioned into a single response variable, $Y$, conditioned on the remainder predictor variables, $X_1, \cdots, X_n$,
\begin{equation}
\mathbf{W} = \begin{pmatrix} Y \\ X_1 \\ \vdots \\ X_m \end{pmatrix} \Rightarrow
\begin{cases}
\boldsymbol{\mu} = \begin{pmatrix} \mu_y \\ \boldsymbol{\mu}_{\mathbf{x}} \end{pmatrix},  \boldsymbol{\mu}_{\mathbf{x}} = \begin{pmatrix} \mu_{x_1} \\ \vdots \\ \mu_{x_m} \end{pmatrix}\\ \\
\boldsymbol{\Sigma} = \begin{pmatrix} \sigma^2_y & \boldsymbol{\sigma}^\top_{y\mathbf{x}} \\ \boldsymbol{\sigma}_{y\mathbf{x}} & \boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}} \end{pmatrix},
\begin{cases}
\boldsymbol{\sigma}_{y\mathbf{x}} = \begin{pmatrix} \sigma_{yx_1} & \cdots & \sigma_{yx_m} \end{pmatrix}^\top \\
\boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}} = 
\begin{pmatrix}
\sigma^2_{x_1} & \cdots & \sigma_{x_1 x_m} \\
\vdots & \ddots & \vdots \\
\sigma_{x_1 x_m} & \cdots & \sigma^2_{x_m}
\end{pmatrix}
\end{cases}
\end{cases}
\end{equation}
then the single-variate conditional distribution is also Gaussian, and takes the form, 
\begin{equation}
Y| (\mathbf{X} = \mathbf{x}) \sim \operatorname{N}\left(\mu_y + (\mathbf{x} - \boldsymbol{\mu}_{\mathbf{x}})^\top \boldsymbol{\Sigma}_{\mathbf{x} \mathbf{x}}^{-1} \boldsymbol{\sigma}_{y\mathbf{x}}, \sigma^2_y - \boldsymbol{\sigma}_{y\mathbf{x}}^\top \boldsymbol{\Sigma}_{\mathbf{x} \mathbf{x}}^{-1} \boldsymbol{\sigma}_{y \mathbf{x}} \right).
\end{equation}

The parameters of the Gaussian distribution can be simplified -- and the linear form of the mean stressed -- by introducing the following constants,
\begin{align}\label{eq:conditional_mean_variance}
\begin{rcases}
\mathbf{a} = \begin{pmatrix} 1 \\ \mathbf{x} \end{pmatrix} \\
\boldsymbol{\alpha} = \begin{pmatrix} \mu_y - \boldsymbol{\sigma}_{y\mathbf{x}}^\top \boldsymbol{\Sigma}^{-1}_{\mathbf{x} \mathbf{x}} \boldsymbol{\mu}_{\mathbf{x}} \\ \boldsymbol{\Sigma}^{-1}_{\mathbf{x} \mathbf{x}} \boldsymbol{\mu}_{\mathbf{x}} \end{pmatrix}
\end{rcases} &\Rightarrow
\mu_{y|\mathbf{x}} = \mu_y + (\mathbf{x} - \boldsymbol{\mu}_{\mathbf{x}})^\top \boldsymbol{\Sigma}_{\mathbf{x} \mathbf{x}}^{-1} \boldsymbol{\sigma}_{y\mathbf{x}} \equiv \boldsymbol{\alpha}^\top \mathbf{a} \\
R^2 = \frac{\boldsymbol{\sigma}_{y\mathbf{x}}^\top \boldsymbol{\Sigma}_{\mathbf{x} \mathbf{x}}^{-1} \boldsymbol{\sigma}_{y \mathbf{x}}}{\sigma^2_y} \hspace{34pt} &\Rightarrow \sigma_{y|\mathbf{x}}^2 = \sigma^2_y \left(1 - R^2 \right)
\end{align}
which leads to the expression,
\begin{equation}\label{eq:conditional_distribution}
Y|(\mathbf{X} = \mathbf{x}) \sim \operatorname{N}(\mu(\mathbf{x}), \sigma^2) \equiv \operatorname{N} \left(\boldsymbol{\alpha}^\top \mathbf{a},  \sigma^2_y \left(1 - R^2 \right) \right).
\end{equation}

Here, the parameter, $R^2$, which appears in the variance of the conditional distribution, is the familiar coefficient of determination.  It is clear from the derivation provided above, however, that the parameter describes the strength of the linear relationship between predictor and response variables, and is not an independent property of the measurement data.

\subsection{Estimation of Linear Relationship from Data}
The linear relationship between response and predictor variables is derived from information contained within the matrix, $W$, which contains a list of indexed measurements,
\begin{equation}
W = \begin{pmatrix} y_1 & x_{11} & \cdots & x_{1m} \\
\vdots & \vdots & \ddots & \vdots \\
y_n & x_{n1} & \cdots & x_{nm} \end{pmatrix} =
\begin{pmatrix}  \mathbf{y} & X \end{pmatrix}.
\end{equation}
Here, the measurements are organized by row, in which the first column contains the response variable, $\mathbf{y}$, and the remaining columns contain the predictors, $X$, and is called the \textbf{data matrix}.  The predictor data points may be selected by design, or by chance.

We construct the \textbf{extended data matrix} by replacing the response vector with a unit vector, 
\begin{equation}\label{eq:extended_data_matrix}
A = \begin{pmatrix} 1 & x_{11} & \cdots & x_{1m} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & \cdots & x_{nm} \end{pmatrix} =
\begin{pmatrix}  \mathbf{1} & X \end{pmatrix},
\end{equation}
which contains all information from which the linear relationship is to be derived.  Each row in the extended data matrix in (\ref{eq:extended_data_matrix}) matches the form of the vector of arbitrary predictor measurements in the conditional mean show above in (\ref{eq:conditional_mean_variance}).

\begin{equation}
\mathbf{a}_i^\top = \begin{pmatrix} 1 & x_{i1} & \cdots & x_{im}  \end{pmatrix} = \begin{pmatrix} 1 & \mathbf{x}^\top_i \end{pmatrix} \Rightarrow
A = \begin{pmatrix} \mathbf{a}^\top_1 \\ \vdots \\ \mathbf{a}^\top_n \end{pmatrix}
\end{equation}

\subsection{Engineering Approach: Least Squares}\label{sec:linear_model_engineering}
The engineering approach to linear modeling is quite direct: assert a linear relationship between the vector of response variables, $\mathbf{y}$, and the extended data matrix, $A$, in which the linear relationship is mediated through a vector of coefficients, $\boldsymbol{\alpha}$,
\begin{equation}\label{eq:linear_predictor_response}
\mathbf{y} - A \boldsymbol{\alpha} = 0.
\end{equation}
Typically, the number of data points, $n$, exceeds the number of predictors, $m$, and the linear relation in (\ref{eq:linear_predictor_response}) is overdetermined.  The properties of solutions to linear equations are covered in \textit{LAN} $\S$7.

Although there is generally no \textit{exact} solution to the equation in (\ref{eq:linear_predictor_response}), we can determine the \textit{best} solution given a condition of optimality.  For a quadratic penalty function the optimization problem leads to a gradient operator applied to an inner product:
\begin{equation}\label{eq:least_squares}
\hat{\boldsymbol{\alpha}} \leftarrow \min_{\boldsymbol{\alpha}} || \mathbf{y} - A \boldsymbol{\alpha} ||_2^2 \Rightarrow \nabla_{\boldsymbol{\alpha}} \left( \mathbf{y} - A \boldsymbol{\alpha} \right)^\top\left( \mathbf{y} - A \boldsymbol{\alpha} \right) = 0.
\end{equation}
This problem, covered also in \textit{LAA} $\S$3, is solved by expanding the inner product and applying the gradient operator term by term:
\begin{align}\label{eq:least_squares_details}
\nabla_{\boldsymbol{\alpha}} \left( \mathbf{y} - A \boldsymbol{\alpha} \right)^\top\left( \mathbf{y} - A \boldsymbol{\alpha} \right) & =
\nabla_{\boldsymbol{\alpha}} \left( A \boldsymbol{\alpha} \right)^\top \left( A \boldsymbol{\alpha} \right) - \nabla_{\boldsymbol{\alpha}} \left( A \boldsymbol{\alpha} \right)^\top \mathbf{y} - \nabla_{\boldsymbol{\alpha}} \mathbf{y}^\top A \boldsymbol{\alpha} + \nabla_{\boldsymbol{\alpha}} \mathbf{y}^\top \mathbf{y} \notag \\
& = \nabla_{\boldsymbol{\alpha}} \boldsymbol{\alpha}^\top A^\top A \boldsymbol{\alpha} - \nabla_{\boldsymbol{\alpha}} \boldsymbol{\alpha}^\top A^\top \mathbf{y} - \nabla_{\boldsymbol{\alpha}} \mathbf{y}^\top A \boldsymbol{\alpha} + \nabla_{\boldsymbol{\alpha}} \mathbf{y}^\top \mathbf{y} \notag \\
& = 2 A^\top A \hat{\boldsymbol{\alpha}} - 2 A^\top \mathbf{y} = 0 \notag \\
\Rightarrow \hat{\boldsymbol{\alpha}} &= \left( A^\top A \right)^{-1} A^\top \mathbf{y}.
\end{align}
Notice that the least-squares solution is achieved by applying a linear operator derived from the extended data matrix, $(A^\top A)^{-1} A^\top$, to the predictor vector, $\mathbf{y}$.  The solution is simply a linear combination of all measurements.

\subsection{Probabilistic Approach: Maximum Entropy}\label{sec:linear_model_probabilistic}
The probabilistic approach to linear modeling asserts that each measurement is drawn from the same underlying distribution,
\begin{equation}\label{eq:distribution_single_sample}
Y| (\mathbf{X} = \mathbf{x}) \sim  \operatorname{N} \left(\boldsymbol{\alpha}^\top \mathbf{a}, \sigma^2 \right) \Rightarrow p_{Y|\mathbf{X}=\mathbf{x}} =\frac{1}{\sigma \sqrt{2 \pi}}  e^{-\frac{1}{2 \sigma^2}\left(y - \boldsymbol{\alpha}^\top \mathbf{a}\right)^2}
\end{equation}
for which the conditional distribution is taken as Gaussian, the linear relation is encoded in the mean, $\boldsymbol{\alpha}^\top \mathbf{a}$, and the strength of the relationship is encoded in the variance, $\sigma^2$.  The determination of the coefficients to the linear model, $\boldsymbol{\alpha}$, is therefore point-estimation problem given sampled data, which is covered in \textit{SN,} $\S$3, while maximum-likelihood estimators are addressed specifically in $\S$3.3.

If each sample is governed by the one-dimensional Gaussian distribution in (\ref{eq:distribution_single_sample}), the full set of samples, assuming each is independent of the other, is governed by the multivariate Gaussian,
\begin{equation}\label{eq:distribution_full_sample}
\begin{rcases}
\mathbf{Y} = \begin{pmatrix} Y_1 & \cdots & Y_n \end{pmatrix}^\top \\
\mathbf{X} = \begin{pmatrix} \mathbf{X}_1 & \cdots & \mathbf{X}_n \end{pmatrix}^\top
\end{rcases}
\Rightarrow
\mathbf{Y} | (\mathbf{X} = \begin{pmatrix} \mathbf{x}_1 & \cdots & \mathbf{x}_n \end{pmatrix}^\top ) = \prod_{i = 1}^n \operatorname{N} \left( \boldsymbol{\alpha}^\top \mathbf{a}_i, \sigma^2 \right) = \operatorname{N}(A \boldsymbol{\alpha}, \sigma^2 I_n).
\end{equation}
Point estimators are derived from operations performed on the joint distribution; point estimators for the coefficients in the linear model, and the variance of the distribution are presented in the next few sections.

\subsubsection{Conditional Mean}

\paragraph{Maximum-Likelihood Estimator} $\\ \\$
The likelihood function for the sample distribution, which is also Shannon information, is simply the logarithm of the joint probability density function (\textit{cf. PN,} $\S$10.4),
\begin{multline}\label{eq:conditional_likelihood_function}
p_{\mathbf{Y} | \mathbf{X}} = \frac{1}{\sigma^n\sqrt{(2 \pi)^n}}e^{-\frac{1}{2\sigma^2}(\mathbf{y} - A\boldsymbol{\alpha})^\top(\mathbf{y} - A\boldsymbol{\alpha})} \\
 \Rightarrow \ln p_{\mathbf{Y} | \mathbf{X}} = -\frac{n}{2} \ln 2\pi - \frac{n}{2} \ln \sigma^2 - \frac{1}{2\sigma^2}(\mathbf{y} - A\boldsymbol{\alpha})^\top(\mathbf{y} - A\boldsymbol{\alpha})
\end{multline}
Since the linear model coefficients enter the likelihood function as an inner product, the maximum-likelihood estimator is generated by the extrema of the gradient operator, 
\begin{equation}
\nabla_{\boldsymbol{\alpha}} \left( \mathbf{y} - A \boldsymbol{\alpha} \right)^\top\left( \mathbf{y} - A \boldsymbol{\alpha} \right) = 0 \equiv \hat{\boldsymbol{\alpha}} \leftarrow \min_{\boldsymbol{\alpha}} || \mathbf{y} - A \boldsymbol{\alpha} ||_2^2
\Rightarrow \hat{\boldsymbol{\alpha}} = (A^\top A)^{-1} A^\top \mathbf{y}.
\end{equation}
The optimization condition is identical to the least-squares problem shown above in $\S$\ref{sec:linear_model_engineering}, and so the solutions exactly coincide.

The probabilistic approach holds an advantage over the engineering approach, however, since the statistical parameters of the underlying joint distribution define a distribution for the parameters of the linear model.  Indeed, given the mean and variance of the sample conditional distribution,
\begin{align}
\mathbb{E}_{\mathbf{X}} \mathbf{Y} = A \boldsymbol{\alpha} \\
\mathbb{V}_{\mathbf{X}} \mathbf{Y} = \sigma^2 I
\end{align}
the mean and variance of the estimated coefficients, $\boldsymbol{\alpha}$, can be calculated directly,
\begin{align}
&\mathbb{E}_{\mathbf{X}} \hat{\boldsymbol{\alpha}} =  \mathbb{E}_{\mathbf{X}} (A^\top A)^{-1} A^\top \mathbf{Y} = (A^\top A)^{-1} A^\top \mathbb{E}_{\mathbf{X}} \mathbf{Y} = (A^\top A)^{-1} A^\top A \boldsymbol{\alpha} = \boldsymbol{\alpha}, \\
&\mathbb{V}_{\mathbf{X}} \hat{\boldsymbol{\alpha}} = \mathbb{V}_{\mathbf{X}} (A^\top A)^{-1} A^\top \mathbf{Y} = (A^\top A)^{-1} A^\top (\mathbb{V}_{\mathbf{X}} \mathbf{Y} ) A (A^\top A)^{-1} = (A^\top A)^{-1} A^\top (\sigma^2 I) A (A^\top A)^{-1} \notag \\
&\hspace{260pt}= \sigma^2 (A^\top A)^{-1}.
\end{align}

Finally, since the underlying conditional distribution is Gaussian, and all arithmetic operations to generate the coefficients are linear, the \textit{distribution} of the estimated coefficients must also be Gaussian (\textit{cf. PN}, $\S$7.2.1.4), and is given by 
\begin{equation}\label{eq:linear_parameters_distributions}
\hat{\boldsymbol{\alpha}} \sim \operatorname{N} \left( \boldsymbol{\alpha}, \sigma^2 (A^\top A)^{-1} \right).
\end{equation}


\subsubsection{Conditional Variance}

\paragraph{Maximum-Likelihood Estimator} $\\ \\$
The maximum-likelihood estimator for the variance of the conditional distribution, whose likelihood function is provided in (\ref{eq:conditional_likelihood_function}), is the extremum of the derivative with respect to variance,
\begin{multline}\label{eq:maximum_likelihood_variance_estimator}
\hspace{10pt} \frac{\partial}{\partial \sigma^2} \left( -\frac{n}{2} \ln \sigma^2  -\frac{1}{2 \sigma^2} \left( \mathbf{y} - A \boldsymbol{\alpha} \right)^\top\left( \mathbf{y} - A \boldsymbol{\alpha} \right) \right) = 0 \\
\Rightarrow \hat{\sigma}^2 
= \frac{1}{n} \left( \mathbf{y} - A \hat{\boldsymbol{\alpha}} \right)^\top\left( \mathbf{y} - A \hat{\boldsymbol{\alpha}} \right) 
= \frac{1}{n} \left( \mathbf{y} - A (A^\top A)^{-1} A^\top \mathbf{y} \right)^\top\left( \mathbf{y} - A (A^\top A)^{-1} A^\top \mathbf{y} \right) \\
= \frac{1}{n} \left( \mathbf{y} - P_ A \mathbf{y} \right)^\top\left( \mathbf{y} - P_A \mathbf{y} \right) 
= \frac{1}{n} \, \mathbf{y}^\top \left( I -P_A \right) \mathbf{y} \hspace{149pt}
\end{multline}
The maximum-likelihood variance estimator is a quadratic form with the matrix operator, $I - P_A$, that projects vectors into the orthogonal complement of the column space of the extended data matrix, $A$. 

\paragraph{Unbiased Estimator} $\\ \\$
 The quadratic form in (\ref{eq:maximum_likelihood_variance_estimator}) shows that information in the variance is contained within the orthogonal complement of the column space of the data matrix, which is an $n-(m+1)$-dimensional subspace.  Coupled with the observation that the mean is projected into the column space,
\begin{equation}
A\hat{\boldsymbol{\alpha}} = A (A^\top A)^{-1} A^\top \mathbf{y} = P_A \mathbf{y},
\end{equation}
which is an $(m+1)$-dimensional subspace, it is clear that the maximum-entropy estimators for the conditional Gaussian distribution generalize Fisher's Theorem (\textit{cf. PN}, $\S$7.2.1.7) to a multidimensional setting.  The arguments that hold in the 1-dimensional case carry over to the multidimensional case, since
\begin{equation}
\begin{rcases}
\mathbb{E}_{\mathbf{X}} \left( \mathbf{Y}^\top \mathbf{Y} \right) = n \sigma^2 \\
\mathbb{E}_{\mathbf{X}} \left( \mathbf{Y}^\top P_A \mathbf{Y} \right) = (m + 1) \sigma^2 
\end{rcases} \Rightarrow
\mathbb{E}_{\mathbf{X}} \left( \frac{1}{n - (m+1)} \mathbf{Y}^\top \left(I - P_A  \right) \mathbf{Y}  \right)= \sigma^2.
\end{equation}
We can therefore adjust the normalizing factor in the maximum-likelihood variance estimator, which minimizes the overall average discrepancy between the estimated and actual variances, to derive a least-unbiased estimator,
\begin{equation}
\hat{s}^2 =  \frac{1}{n - (m+1)} \mathbf{y}^\top \left(I - P_A  \right) \mathbf{y}.
\end{equation}

The distribution of the scaled unbiased estimator is then a chi-squared distribution with the appropriate degrees of freedom,
\begin{equation}\label{eq:unbiased_variance_estimator}
(n - (m+1)) \frac{\hat{s}^2}{\sigma^2} \sim \chi^2_{n-(m+1)}.
\end{equation}

\paragraph{Interval Tests for Linear Coefficient Estimators} $\\ \\$
Given that the 

\begin{equation}
\frac{\mathbf{r}^\top (\hat{\boldsymbol{\alpha}} - \boldsymbol{\alpha})}{\sqrt{\hat{s}^2 \mathbf{r}^\top (A^\top A)^{-1} \mathbf{r}}} \sim T(n-(m+1)
\end{equation}




\begin{equation}
\mathbf{r} \equiv \mathbf{e}_i \Rightarrow \frac{\hat{\alpha}_i - \alpha_i}{\sqrt{\hat{s}^2 (A^\top A)^{-1}_i}} \sim T(n-(m+1))
\end{equation}



\subsubsection{Linear Operations of Least Squares and Maximum-Entropy Sample Statistics}
The least-squares and maximum-entropy methods generate identical linear-algebraic operations and solutions.  We show here that the arithmetic operations carried out in the matrix formulae encode detailed sample estimators for every mean, variance and covariance in the underlying joint distribution.  The solutions are both equivalent to replacing all statistical quantities with sample estimates, and the matrix operations shown above are exactly those necessary to achieve this.

Given the extended data matrix, $A$, the \textbf{scatter matrix}, $A^\top A$, can be expressed as
\begin{equation}\label{eq:partitioned_scatter_matrix}
A^\top A = \begin{pmatrix} \mathbf{1}^\top \\ X^\top \end{pmatrix} \begin{pmatrix} \mathbf{1} & X \end{pmatrix} 
= \begin{pmatrix} \mathbf{1}^\top \mathbf{1} & \mathbf{1}^\top X \\ 
X^\top \mathbf{1} & X^\top X.
\end{pmatrix}
\end{equation}
Each of these operations encode sample estimators for mean and covariance statistics (\textit{cf. PN}, $\S$3.3),
\begin{align}
&\mathbf{1}^\top \mathbf{1} = n, \\
&\mathbf{1}^\top X =  n \hat{\boldsymbol{\mu}}^\top_{\mathbf{x}}, \\
&X^\top \mathbf{1} = n \hat{\boldsymbol{\mu}}_{\mathbf{x}}, \\
&X^\top X = n\left(\hat{\boldsymbol{\Sigma}}_{\mathbf{x}\mathbf{x}} +  \hat{\boldsymbol{\mu}}_{\mathbf{x}}\hat{\boldsymbol{\mu}}^\top_{\mathbf{x}}\right).
\end{align}

The partitioned scatter matrix in (\ref{eq:partitioned_scatter_matrix}) can be inverted by use of the lower Schur block matrix inversion formula (\textit{cf. LAN}, $\S$6.2), repeated here as (note the unrelated use of the variable, $A$),
\begin{equation}\label{eq:block_matrix_inverse_lower}
\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}^{-1} = 
\begin{pmatrix}
A^{-1} + A^{-1}B \left( D - CA^{-1}B \right)^{-1} CA^{-1} & -A^{-1}B \left( D - CA^{-1}B \right)^{-1} \\
-\left( D - CA^{-1}B \right)^{-1}CA^{-1} & \left( D - CA^{-1}B \right)^{-1}
\end{pmatrix}.
\end{equation}

Upon assigning the sample statistics in the scatter matrix to the blocks in the Schur formula, we directly construct the inverse of the scatter matrix,
\begin{equation}
\begin{rcases}
A = n \\
B = n \hat{\boldsymbol{\mu}}^\top_{\mathbf{x}} \\
C = n \hat{\boldsymbol{\mu}}_{\mathbf{x}} \\
D = n \left(\hat{\boldsymbol{\Sigma}}_{\mathbf{x}\mathbf{x}} +  \hat{\boldsymbol{\mu}}_{\mathbf{x}}\hat{\boldsymbol{\mu}}^\top_{\mathbf{x}} \right)
\end{rcases} \Rightarrow
\begin{pmatrix}
A^\top A
\end{pmatrix}^{-1}
= \frac{1}{n}
\begin{pmatrix}
1 + \hat{\boldsymbol{\mu}}^\top_{\mathbf{x}}\hat{\boldsymbol{\Sigma}}_{\mathbf{x}\mathbf{x}}^{-1} \hat{\boldsymbol{\mu}}_{\mathbf{x}} & -\hat{\boldsymbol{\mu}}^\top_{\mathbf{x}}\hat{\boldsymbol{\Sigma}}_{\mathbf{x}\mathbf{x}}^{-1} \\
-\hat{\boldsymbol{\Sigma}}_{\mathbf{x}\mathbf{x}}^{-1} \hat{\boldsymbol{\mu}}_{\mathbf{x}} & \hat{\boldsymbol{\Sigma}}_{\mathbf{x}\mathbf{x}}^{-1}
\end{pmatrix}
\end{equation}

Similarly, the means and covarances of the response variables are carried out by linear operation with the data matrix,
\begin{equation}
A^\top \mathbf{y} = \begin{pmatrix} \mathbf{1}^\top \mathbf{y} \\ X^\top \mathbf{y} \end{pmatrix} = n
\begin{pmatrix} \hat{\mu}_y \\ \hat{\boldsymbol{\sigma}}_{y\mathbf{x}} + \hat{\mu}_y  \hat{\boldsymbol{\mu}}_{\mathbf{x}} \end{pmatrix}.
\end{equation}

Applying these results to the operations for calculating estimates of the conditional mean and variance
\begin{align}
&\hat{\boldsymbol{\alpha}} = (A^\top A)^{-1} A^\top \mathbf{y} = 
\begin{pmatrix}
\hat{\mu}_y - \hat{\boldsymbol{\mu}}^\top_{\mathbf{x}}\hat{\boldsymbol{\Sigma}}_{\mathbf{x}\mathbf{x}}^{-1} \hat{\boldsymbol{\sigma}}_{y\mathbf{x}} \\ \hat{\boldsymbol{\Sigma}}_{\mathbf{x}\mathbf{x}}^{-1} \hat{\boldsymbol{\sigma}}_{y\mathbf{x}}
\end{pmatrix} \\
&\hat{\sigma}^2 = \frac{1}{n}\left( \mathbf{y}^\top \mathbf{y} - \mathbf{y}^\top A (A^\top A)^{-1} A^\top \mathbf{y} \right)= \hat{\sigma}^2_y - \hat{\boldsymbol{\sigma}}_{y \mathbf{x}}^\top \hat{\boldsymbol{\Sigma}}^{-1}_{\mathbf{x} \mathbf{x}} \hat{\boldsymbol{\sigma}}_{y \mathbf{x}} = \hat{\sigma}^2_y \left( 1 - \hat{R}^2 \right)
\end{align}
we generate exactly the results achieved by replacing the statistical parameters in the conditional distribution in (\ref{eq:conditional_distribution}) with sample estimates.

Finally, the distribution of the coefficients of the linear model can be expressed as
\begin{equation}
\hat{\boldsymbol{\alpha}} \sim \operatorname{N} \left( \begin{pmatrix} \mu_y - \boldsymbol{\sigma}_{y\mathbf{x}}^\top \boldsymbol{\Sigma}^{-1}_{\mathbf{x} \mathbf{x}} \boldsymbol{\mu}_{\mathbf{x}} \\ \boldsymbol{\Sigma}^{-1}_{\mathbf{x} \mathbf{x}} \boldsymbol{\mu}_{\mathbf{x}} \end{pmatrix}, \frac{\sigma^2}{n} 
\begin{pmatrix}
1 + \boldsymbol{\mu}^\top_{\mathbf{x}}\boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}}^{-1} \boldsymbol{\mu}_{\mathbf{x}} & -\boldsymbol{\mu}^\top_{\mathbf{x}}\boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}}^{-1} \\
-\boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}}^{-1} \boldsymbol{\mu}_{\mathbf{x}} & \boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}}^{-1}
\end{pmatrix}
\right).
\end{equation}

\subsection{Sensitivity of the Linear Model}

\subsubsection{Condition Number}\label{sec:linear_model_condition_number}
Much as the matrix condition number, (\textit{cf. LAN,} $\S$7.2), measures the stability of the solution to a linear equation in (\textit{LAN,} (121)), the\textbf{ least-squares condition number}, $\kappa_{LS}$, measures the stability of the overdetermined system above in (12).  There are two sources of potential instability, including

\begin{itemize}[noitemsep]
\item a dependency on the strength of the relationship between response and predictor variables, $\hat{R}$;
\item a dependency on the condition number of the data matrix, $\kappa(A)$.
\end{itemize} 

Given the solution to the linear model, $\boldsymbol{\alpha} = \boldsymbol{\alpha}_{LS}$, and perturbations to the matrix and target vector, $\delta A$ and $\delta \mathbf{y}$, respectively, the condition number, $\kappa_{LS}(A, \mathbf{y})$, is expressed as the relative magnitude of the perturbation to the solution:
\begin{equation}\label{eq:perturbed_least_squares}
\begin{rcases}
\boldsymbol{\alpha}_{LS} \stackrel{\operatorname{min}}{\longleftarrow} A \boldsymbol{\alpha} = \mathbf{y} \\
(A + \delta A) (\boldsymbol{\alpha} + \delta \boldsymbol{\alpha}) = \mathbf{y} + \delta \mathbf{y}
\end{rcases} \Rightarrow
\frac{||\delta \boldsymbol{\alpha}||_2}{|| \boldsymbol{\alpha}_{LS} ||_2} \leq \kappa_{LS}(A, \mathbf{y}) \max\left( \frac{|| \delta \mathbf{y} ||_2}{|| \mathbf{y} ||_2}, \frac{|| \delta A ||_2}{|| A ||_2} \right).
\end{equation}

In order to ensure non-singularity in the perturbed equation the relative magnitudes of the perturbations to matrix and target vector are bounded,
\begin{equation}
\epsilon = \operatorname{max}\left( \frac{|| \delta A ||_2}{|| A ||_2}, \frac{|| \delta \mathbf{y} ||_2}{ || \mathbf{y} ||_2 }  \right) < \frac{| \sigma_{\operatorname{max}} |}{| \sigma_{\operatorname{min}} |},
\end{equation}
and the least-squares condition number takes the form,
\begin{equation}\label{eq:least_squares_condition_number}
\kappa_{LS}(A, \mathbf{y}) = \kappa_2(A) (1  + \hat{R}^{-1}) + \kappa^2_2(A)(1 - \hat{R}^{-2})^{\frac{1}{2}}.
\end{equation}


To derive the relation in (\ref{eq:least_squares_condition_number}) we parametrize the perturbed least-squares problem in terms of a continuous variable, $t$, for which the finite-level expression in (\ref{eq:perturbed_least_squares}) is captured at the point, $t=\epsilon$:
\begin{equation}
\begin{rcases}
E = \epsilon^{-1} \delta A \\
\mathbf{f} = \epsilon^{-1} \delta \mathbf{y}
\end{rcases} \Rightarrow
(A + \delta A) (\boldsymbol{\alpha} + \delta \boldsymbol{\alpha}) = \mathbf{y} + \delta \mathbf{y} \Rightarrow
\left. [(A + t E) \boldsymbol{\alpha}(t) = \mathbf{y} + tf] \right|_{t=\epsilon},
\end{equation}
and the perturbation to the solution is determined by the derivative to the parametrized equation,
\begin{equation}
\boldsymbol{\alpha} (t)|_{t=\epsilon} \approx \boldsymbol{\alpha}(0) + \epsilon \dot{\boldsymbol{\alpha}} (0) = \boldsymbol{\alpha}_{LS} + \delta \boldsymbol{\alpha}.
\end{equation}
Taking the derivative of the parametrized equation and evaluating the result at the origin yields the result,
\begin{multline}
\left. \frac{d}{dt} [(A + tE)^\top (A + tE) \boldsymbol{\alpha}(t) = (A + tE)^\top (\mathbf{y} + t\mathbf{f})]\right|_{t=\epsilon} \Rightarrow \\
\dot{\boldsymbol{\alpha}}(0) = (A^\top A)^{-1} E^\top (\mathbf{y} - A \boldsymbol{\alpha}_{LS}) + (A^\top A)^{-1} A^\top (\mathbf{f } -  E \boldsymbol{\alpha}_{LS}).
\end{multline}

Given the bounds
\begin{align}
&|| E ||_2 \leq ||A||_2 \\
&||\mathbf{f}||_2 \leq ||\mathbf{y} ||_2 \\
&\frac{1}{||A||_2 \; ||\boldsymbol{\alpha}_{LS } ||_2} \leq \frac{1}{||A \boldsymbol{\alpha}_{LS } ||_2} = \frac{1}{|| P_A \mathbf{y} ||_2}
\end{align}
and the relations, which are easily established by expanding the matrices in terms of singular values, ((\textit{cf. LAN,} $\S$5.1), and the  matrix condition number, (\textit{cf. LAN,} $\S$7.2),
\begin{align}
&||A||_2 \; ||(A^\top A)^{-1} A^\top ||_2 = |\sigma_{\operatorname{max}}| |\sigma^{-1}_{\operatorname{min}}| \hspace{9pt}= \kappa_2(A) \\
&||A||^2_2 \; ||(A^\top A)^{-1} ||_2 \hspace{14pt} =|\sigma_{\operatorname{max}}|^2 |\sigma^{-1}_{\operatorname{min}}|^{2} = \kappa_2^2(A)
\end{align}
we can take the norm of each relative term in the derivative:
\begin{multline}
\frac{||(A^\top A)^{-1} ||_2 \; ||E||_2 \; || \mathbf{y} - A \boldsymbol{\alpha}_{LS} ||_2}{|| \boldsymbol{\alpha}_{LS} ||_2} \leq \frac{ ||(A^\top A)^{-1} ||_2 \; ||A||_2 \; || (I - P_A) \mathbf{y}||_2}{|| \boldsymbol{\alpha}_{LS} ||_2} \leq \\
||(A^\top A)^{-1} ||_2 \; ||A||^2_2 \; \frac{|| (I - P_A) \mathbf{y}||_2}{|| A ||_2 \; || \boldsymbol{\alpha}_{LS} ||_2} \leq ||(A^\top A)^{-1} ||_2 \; ||A||^2_2 \; \frac{|| (I - P_A) \mathbf{y}||_2}{|| P_A \mathbf{y} ||_2} = \kappa_2^2(A) (1 - \hat{R}^{-2})^\frac{1}{2},
\end{multline}
and
\begin{multline}
\frac{|| (A^\top A)^{-1} A^\top ||_2 \; (|| \mathbf{f} ||_2 + || E ||_2 \; ||\boldsymbol{\alpha}_{LS}||_2 )}{|| \boldsymbol{\alpha}_{LS} ||_2} \leq || (A^\top A)^{-1} A^\top ||_2 \; \left( \frac{ || \mathbf{y} ||_2}{|| \boldsymbol{\alpha}_{LS} ||_2} + || A ||_2 \  \right)\leq \\
|| (A^\top A)^{-1} A^\top ||_2 \; ||A||_2 \; \left(\frac{ || \mathbf{y} ||_2 }{ || A ||_2 \; || \boldsymbol{\alpha}_{LS} ||_2}+ 1 \right)
\leq || (A^\top A)^{-1} A^\top ||_2 \; ||A||_2 \; \left(\frac{ || \mathbf{y} ||_2 }{ || P_A \mathbf{y} ||_2}+ 1 \right) \\ 
= \kappa_2(A) (1 + \hat{R}^{-1}).
\end{multline}

Substituting these values in the definition for the condition number yields the desired result,
\begin{equation}
\kappa_{LS}(A, \mathbf{y}) \equiv \frac{|| \dot{\boldsymbol{\alpha}}(0) ||_2}{|| \boldsymbol{\alpha}(0) ||_2} = \frac{|| \delta \boldsymbol{\alpha} ||_2}{|| \boldsymbol{\alpha}_{LS} ||_2} = \kappa_2(A) (1  + \hat{R}^{-1}) + \kappa^2_2(A)(1 - \hat{R}^{-2})^{\frac{1}{2}}.
\end{equation}

Finally, note that the least-squares condition number is dominated by different powers of the matrix condition number for the two asymptotic bounds of the coefficient of determination, $\hat{R}$:
\begin{align}
\hat{R} \rightarrow 1 &\Rightarrow \kappa_{LS}(A, \mathbf{y}) \rightarrow 2 \kappa_2(A) \\
\hat{R} \rightarrow 0 &\Rightarrow \kappa_{LS}(A, \mathbf{y}) \rightarrow \kappa_2^2(A) \hat{R}^{-1}.
\end{align}


\subsubsection{Collinearity Diagnostics}\label{sec:Linear_Model_Collinearity_Diagnostics}
Although the least-squares condition number, described above in $\S$\ref{sec:linear_model_condition_number}, provides a measure of the sensitivity of the solution to the linear model, it is insufficient to diagnose the specific causes of or specific contributions to instability.  In particular it is insufficient to determine which variates might jointly contribute to instability through collinearity.  In this section we present a collinearity diagnostic that
\begin{itemize}[noitemsep]
\item provides clear thresholds for the presence of collinearity;
\item identifies all variates that participate in a single collinear relationship;
\item distinguishes between variate participation in multiple collinear relationships.
\end{itemize}

The matrix condition number, which figures strongly in the least-squares condition number, is affected both by the relative magnitudes and relative positions of the columns of the matrix, $A$.  Collinearity, however, is a measure of the position of the column vectors alone, and the collinearity diagnostic operates on an adjusted matrix whose column vectors are scaled to unit magnitude:
\begin{equation}
\tilde{A} = AS \Rightarrow \tilde{\mathbf{a}}_i^\top \tilde{\mathbf{a}}_i = 1,
\end{equation}
for which $S$ is the appropriate diagonal matrix.

The collinearity diagnostics take two forms:
\begin{itemize}[noitemsep]
\item the \textbf{condition indices}, which are the relative magnitude of the singular values of the scaled matrix:
\begin{equation}
\tilde{\eta}_i \equiv \frac{|\tilde{\sigma}_i|}{|\tilde{\sigma}_{\operatorname{min}}|}
\end{equation}
\item the \textbf{variance-decomposition} matrix, $\Pi$,  for which $(i,j)^{th}$ entry, $\pi_{ij}$, is the normalized contribution of the $j^{th}$ variate to the variance of the $i^{th}$ coefficient, $(\mathbb{V} \tilde{\boldsymbol{\alpha}})_i \equiv (\tilde{A}^\top \tilde{A})^{-1}_i$.  Using the Singular Value Decomposition, (\textit{cf. LAN,} $\S$5.1) for the scaled data matrix, $\tilde{A}$, we have
\begin{equation}
\tilde{A} = \tilde{U} \tilde{\Sigma} \tilde{V}^\top \Rightarrow (\tilde{A}^\top \tilde{A})^{-1} = \tilde{V} \tilde{\Sigma}^{-2} \tilde{V}^\top \Rightarrow \pi_{ij} = \frac{\tilde{v}_{ij}^2 \tilde{\sigma}_i^{-2}}{\sum_{j} \tilde{v}_{ij}^2 \tilde{\sigma}_j^{-2}}.
\end{equation}
\end{itemize}

The presence of collinearity in the linear model is diagnosed by the following properties of condition indices and the variance-decomposition matrix:
\begin{itemize}[noitemsep]
\item `large' condition indices, together with
\item two or more entries in the variance-decomposition matrix with at least one matching column index.
\end{itemize}

Depending on the the properties of data and model, the collinearity issues might be addressed by removing the appropriate contributors.  Extensive discussion on diagnosing and correcting for collinearity, including detailed critiques of popular alternative methods, is provided in \textit{Conditioning Diagnostics}.


\subsection{Glossary of Statistical Terms Used in Linear Regression}
Many presentations of linear regression are rooted in statistics, and employ standard statistical terms in describing problems, ranges and solutions.


\begin{align}
& \textbf{Statistical Term} && \textbf{Acronym} && \textbf{Pointwise Formula} && \textbf{Linear Algebraic Formula} \notag\\
&\text{Total Sum of Squares} && \text{TSS} && \sum_{i=1}^n (y_i - \bar{y})^2 && \mathbf{y}^\top (I_n - P_{\mathbf{1}_n}) \mathbf{y} \\
&\text{Residual Sum of Squares} && \text{RSS} && \sum_{i=1}^n (y_i - \hat{y}_i)^2 && (\mathbf{y} - A\hat{\boldsymbol{\alpha}})^\top (\mathbf{y} - A\hat{\boldsymbol{\alpha}}) \\
&\text{Error Sum of Squares} && \text{SSE} && \sum_{i=1}^n (y_i - \hat{y}_i)^2 \equiv \sum_{i=1}^n e^2_i && \mathbf{y}^\top (I_n - P_{A}) \mathbf{y} \\
&\text{Regression Sum of Squares} && \text{RSE} && \alpha_1 \sum_{i=1}^n (x_i - \bar{x}_i)^2  &&  \alpha_1 \hat{\sigma}_x^2 \\
&\text{Residual Standard Error} && \text{RSE} && \sqrt{\frac{1}{n-(m+1)} \sum_{i=1}^n e_i^2 }&&  \sqrt{\frac{1}{n-(m+1)} \mathbf{y}^\top (I_n - P_{A}) \mathbf{y}} \\
&\text{Standard Error of Estimator} && \text{SE}(\alpha_i) && \textit{depends...} &&  \sqrt{\sigma^2 (A^\top A)^{-1}_i}
\end{align}





\begin{equation}
\begin{rcases}
\hat{\alpha}_0 = \bar{y} - \hat{\alpha}_1 \bar{x} \\
\hat{\alpha}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{rcases} \Rightarrow \hat{\boldsymbol{\alpha}} = (A^\top A)^{-1} A^\top \mathbf{y} = 
\begin{pmatrix}
\hat{\mu}_y - \hat{\boldsymbol{\mu}}^\top_{\mathbf{x}}\hat{\boldsymbol{\Sigma}}_{\mathbf{x}\mathbf{x}}^{-1} \hat{\boldsymbol{\sigma}}_{y\mathbf{x}} \\ \hat{\boldsymbol{\Sigma}}_{\mathbf{x}\mathbf{x}}^{-1} \hat{\boldsymbol{\sigma}}_{y\mathbf{x}}
\end{pmatrix}
\end{equation}

\begin{equation}
\begin{rcases}
\text{SE}(\hat{\alpha}_0)^2 = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\
\text{SE}(\hat{\alpha}_1)^2= \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{rcases} \Rightarrow \operatorname{diag} \left( \frac{\sigma^2}{n} 
\begin{pmatrix}
1 + \boldsymbol{\mu}^\top_{\mathbf{x}}\boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}}^{-1} \boldsymbol{\mu}_{\mathbf{x}} & -\boldsymbol{\mu}^\top_{\mathbf{x}}\boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}}^{-1} \\
-\boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}}^{-1} \boldsymbol{\mu}_{\mathbf{x}} & \boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}}^{-1}
\end{pmatrix} \right)
\end{equation}



\section{Linear Filters}\label{sec:linear_filter}
A sequence of unobservable state vectors, $\mathbf{x}(t_i) \equiv \mathbf{x}_i$, evolving dynamically, is estimated from a matching set of measurements, $\mathbf{z}(t_i) \equiv \mathbf{z}_i$.  The model is executed in two phases -- evolution and measurement -- each subject to characteristic error and uncertainty.

The state and measurement vectors may lie in different domains, but all functional relations are linear, with transitions or transformations mediated through matrices.  And although the state may evolve continuously, information from measurements is received at discrete times, and we can model the process of evolution and measurement in two separate steps:
\begin{align}
\text{Evolution: } & \mathbf{x}_{i} = F_i \mathbf{x}_{i-1} + \mathbf{w}_i \label{eq:linear_filter_evolution}\\
\text{Measurement: } & \mathbf{z}_{i} = H_i \mathbf{x}_i + \mathbf{v}_i \label{eq:linear_filter_measurement}
\end{align}
Here, the uncertainties in evolution and errors in measurement are captured by the noise vectors, $\mathbf{w}$ and $\mathbf{v}$, respectively.

The history of measurements, expressed as the realization of a sequence of random variables,
\begin{equation}
\mathbf{Z}_1^{i-1} \equiv (\mathbf{Z}_{i-1} = \mathbf{z}_{i-1}), \cdots, (\mathbf{Z}_1 = \mathbf{z}_1), 
\end{equation}
is the conditioning for the hidden state vector at time, $t_i$, at which time a new measurement is made, $\mathbf{Z}_i = \mathbf{z}_i$.  The estimators for the hidden state vector can be modeled as the separate random variables,
\begin{align}
\text{Evolution: } & \mathbf{X}_{i|i-1} = \mathbf{X}_{i-1} | \mathbf{Z}_1^{i-1} \\
\text{Measurement: } & \hspace{16pt} \mathbf{X}_i = \mathbf{X}_{i|i-1} | \mathbf{Z}_i,
\end{align}
each of which accounts for the additional information anticipated or received in the discrete two-step process described above in (\ref{eq:linear_filter_evolution}) and (\ref{eq:linear_filter_measurement}).

\subsection{Kalman Filters}\label{sec:Kalman_filter}
If the unobservable initial state vector, $\mathbf{x}_0$, is modeled by a Gaussian distribution, then all subsequent estimations of state take Gaussian form as well, since both evolution and measurement are executed as linear transformations.  Therefore, both state and measurement vectors, $\mathbf{X}$ and $\mathbf{Z}$, can be imbedded in Gaussian probability models, assuming that
\begin{itemize}[noitemsep]
\item The initial hidden state vector, $\mathbf{x}_0$, is modeled as Gaussian; \\
\item the noise vectors, $\mathbf{w}$ and $\mathbf{v}$, are modeled as zero-mean Gaussians, independent both of each other and of themselves at different times:
\begin{equation}
\begin{rcases}
\mathbf{W} \sim \operatorname{N}(0, Q) \\
\mathbf{V} \sim \operatorname{N}(0, R)
\end{rcases} \Rightarrow
\begin{cases}
\mathbb{C}(Q_i, Q_j) = \mathbb{C}(R_i,R_j) = 0, i \neq j \\
\mathbb{C}(Q_i, R_j) = 0, i, j \in \mathbb{N}
\end{cases}
\end{equation}
\end{itemize}

Since all information in multidimensional Gaussian distributions is contained within the mean vectors and covariance matrices, the sequence of state changes is completely captured by the sequence of moments, which are linked by linear operations defined by the coupling matrices, $F$ and $H$, and the noise covariance matrices, $Q$ and $R$.  The Kalman filter is by construction Markov, and the moments of a given iteration depend only on the moments of the prior iteration and the new information captured in the updated matrices and the new measurement.  The Kalman filter identifies the most-probable state of the system with the mean of the Gaussian distribution, and the accuracy of the estimate with the covariance matrix.

\subsubsection{Stages of the Kalman Filter}
Although the underlying evolution of the state vector may evolve continuously, the Kalman filter is executed at discrete times in two stages, with one an evolution of the state, the other a measurement taken from the updated state.  Given a prior state, the inferred posterior state and estimates of uncertainty of the inferred values are derived from the sequence of linear operations described below.
 
\paragraph{Prior Distribution} $\\ \\$
The hidden state variable is assumed to be well-modeled by a Gaussian distribution with known mean vector and covariance matrix,
\begin{equation}\label{eq:model_prior}
\mathbf{X}_i \sim \operatorname{N}(\boldsymbol{\mu}_i, \boldsymbol{\Sigma}_i).
\end{equation}

\paragraph{Evolution Stage} $\\ \\$
The evolution of the hidden state variable is assumed to follow the linear operation described above in (\ref{eq:linear_filter_evolution}),
\begin{equation}
\mathbf{X}_{i+1|i} = F_{i+1} \mathbf{X}_i +\mathbf{W}_{i+1}.
\end{equation}
Linear operations on Gaussian distributions yield other Gaussian distributions whose moments are linear transformations of the original,
\begin{align}
&\boldsymbol{\mu}_{i+1|i} = \mathbb{E} \mathbf{X}_{i+1|i} = \mathbb{E} (F_{i+1} \mathbf{X}_i + \mathbf{W}_{i+1}) = F_{i+1} \boldsymbol{\mu}_i \\
&\boldsymbol{\Sigma}_{i+1|i} = \mathbb{V} \mathbf{X}_{i+1|i} = \mathbb{E} \left( F_{i+1} \mathbf{X}_i + \mathbf{W}_{i+1} - \mathbb{E}(F_{i+1} \mathbf{X}_i + \mathbf{W}_{i+1})\right) \left( F_{i+1} \mathbf{X}_i + \mathbf{W}_{i+1} - \mathbb{E}(F_{i+1} \mathbf{X}_i + \mathbf{W}_{i+1})\right)^\top \notag \\
&\hspace{75pt}= F_{i+1} \left( \mathbb{E}(\mathbf{X}_i - \mathbb{E}\mathbf{X}_i)(\mathbf{X}_i - \mathbb{E}\mathbf{X}_i)^\top \right) F_{i+1}^\top + \mathbb{E} \mathbf{W}_{i+1} \mathbf{W}_i^\top \notag \\
&\hspace{75pt}= F_{i+1} \boldsymbol{\Sigma}_i F_{i+1}^\top + Q_{i+1}.
\end{align}
The updated hidden state variable is modeled as the Gaussian distribution,
\begin{equation}\label{eq:model_evolution}
\mathbf{X}_{i+1|i} \sim \operatorname{N} (F_{i+1} \boldsymbol{\mu}_i, F_{i+1} \boldsymbol{\Sigma}_i F_{i+1}^\top + Q_{i+}) \equiv \operatorname{N}(\boldsymbol{\mu}_{i+1|i} , \boldsymbol{\Sigma}_{i+1|i}).
\end{equation}

\paragraph{Measurement Stage} $\\ \\$
Although the distribution in (\ref{eq:model_evolution}) models the updated state variable, it is hidden from vew, and its values can only be inferred from measurement.  Measurements are modeled by the linear operation described above in (\ref{eq:linear_filter_measurement}),
\begin{equation}
\mathbf{Z}_{i+1} | \mathbf{X}_{i+1|i} = H_{i+1} \mathbf{X}_{i+1|i} + \mathbf{V}_{i+1}
\end{equation}
from which we derive the distribution of measured values,
\begin{equation}\label{eq:model_measurement}
\mathbf{Z}_{i+1} | \mathbf{X}_{i+1|i} \sim \operatorname{N}(H_{i+1} \boldsymbol{\mu}_{i+1|i}, H_{i+1} \boldsymbol{\Sigma}_{i+1|i} H_{i+1}^\top + R_{i+1}).
\end{equation}

\paragraph{Posterior Distribution} $\\ \\$
Finally, the inferred posterior distribution for the updated state vector takes two pieces of information into account: the estimate for the evolution of state and the specific measurement made from the distribution that governs the updated state.  Here, the posterior distribution can be expressed as a conditional distribution of the state vector conditioned on the measurement.  The Kalman filter is a linear filter with all distributions Gaussian, assumptions that exactly match the linear model described above in $\S$\ref{sec:linear_models}.  Therefore, we can imbed both the hidden state distribution and measurement distribution in a joint Gaussian distribution, and derive the posterior as the distribution of hidden state conditioned on the measurement.

A joint multidimensional Gaussian distribution can be partitioned in two blocks, as shown above in (\ref{eq:partitioned_Gaussian}),
\begin{equation}
\begin{pmatrix} \mathbf{X} \\ \mathbf{Z} \end{pmatrix} \sim
\operatorname{N} 
\begin{pmatrix}  
\begin{pmatrix}\mathbb{E} \mathbf{X} \\ \mathbb{E} \mathbf{Z} \end{pmatrix},
\begin{pmatrix} \mathbb{C}(\mathbf{X}, \mathbf{X}) & \mathbb{C}(\mathbf{X}, \mathbf{Z}) \\ \mathbb{C}(\mathbf{Z}, \mathbf{X}) & \mathbb{C}(\mathbf{Z}, \mathbf{Z}) \end{pmatrix}.
\end{pmatrix}
\end{equation}
Identifying the random variables, $\mathbf{X}$ and $\mathbf{Z}$, as representing the hidden state and measurement distributions, respectively, we can fill out the values in the mean vector and covariance matrix from the means and covariances defined in (\ref{eq:model_evolution}) and (\ref{eq:model_measurement}),
\begin{equation}
\begin{rcases}
\mathbf{X} \rightarrow \mathbf{X}_{i+1|i} \\
\mathbf{Z} \rightarrow \mathbf{Z}_{i+1}| \mathbf{X}_{i+1}
\end{rcases} \Rightarrow \\
\begin{cases}
\begin{pmatrix} \mathbb{E}\mathbf{X} \\ \mathbb{E}\mathbf{Z}  \end{pmatrix} = \begin{pmatrix} \boldsymbol{\mu}_{i+1|i} \\ H_i \boldsymbol{\mu}_{i+1|i} \end{pmatrix} \\ \\
\begin{pmatrix} \mathbb{C}(\mathbf{X}, \mathbf{X}) & \mathbb{C}(\mathbf{X}, \mathbf{Z}) \\ \mathbb{C}(\mathbf{Z}, \mathbf{X}) & \mathbb{C}(\mathbf{Z}, \mathbf{Z}) \end{pmatrix} =
\begin{pmatrix} 
\boldsymbol{\Sigma}_{i+1|i} &  \boldsymbol{\Sigma}_{i+1|i} H_i^\top \\
H_i \boldsymbol{\Sigma}_{i+1|i} & H_i \boldsymbol{\Sigma}_{i+1|i} H_i^\top + R_i
\end{pmatrix}
\end{cases}.
\end{equation}

The conditional distribution is therefore given by (\textit{cf. PN}, $\S$7.2.1.6) and the equation in (\ref{eq:conditioned_Gaussian}) above),
\begin{equation}
\mathbf{X} | (\mathbf{Z} = \mathbf{z}) \sim \operatorname{N} \left( \mathbb{E} \mathbf{X} + \mathbb{C}(\mathbf{X}, \mathbf{Z}) \mathbb{C}^{-1}(\mathbf{Z}, \mathbf{Z}) (\mathbf{z} - \mathbb{E} \mathbf{Z}), \mathbb{C}(\mathbf{X}, \mathbf{X}) - \mathbb{C}(\mathbf{X}, \mathbf{Z}) \mathbb{C}^{-1}(\mathbf{Z}, \mathbf{Z}) \mathbb{C}(\mathbf{Z}, \mathbf{X}) \right).
\end{equation}
For the Kalman filter it is convenient to define the `gain matrix',
\begin{equation}\label{eq:gain_matrix}
K(\mathbf{X}, \mathbf{Z}) \equiv  \mathbb{C}(\mathbf{X}, \mathbf{Z}) \mathbb{C}^{-1}(\mathbf{Z}, \mathbf{Z}) \Rightarrow 
K_i \equiv \boldsymbol{\Sigma}_{i+1|i} H_i^\top (H_i \boldsymbol{\Sigma}_{i+1|i} H_i^\top + R_i)^{-1},
\end{equation}
so that the posterior distribution can be expressed as the Gaussian distribution,
\begin{equation}
\mathbf{X}_{i+1} | (\mathbf{Z}_{i+1} = \mathbf{z}_{i+1}) \equiv \mathbf{X}_{i+1} \sim \operatorname{N}\left(\boldsymbol{\mu}_{i+1|i} + K_i(\mathbf{z}_{i+1} - H_i \boldsymbol{\mu}_{i+1|i}),(I - K_i H_i) \boldsymbol{\Sigma}_{i+1|i} \right).
\end{equation}

\paragraph{Summary of Iterated Estimates for Kalman Filter} $\\ \\$
The Kalman filter identifies the estimator for the hidden state as the mean vectors of the Gaussian distributions, and the measure of uncertainty of the estimate as the covariance matrices.  We can summarize the actions required for execution by presenting the mean vectors and covariant matrices at each stage:

prior distribution
\begin{align}
&\hat{\mathbf{x}}_i = \mathbb{E} \mathbf{X}_i = \boldsymbol{\mu}_i \\
&P_i = \mathbb{V} \mathbf{X}_i = \boldsymbol{\Sigma}_i
\end{align}

evolution distribution
\begin{align}
&\hat{\mathbf{x}}_{i+1|i} = \mathbb{E}\mathbf{X}_{i+1|i} = \boldsymbol{\mu}_{i+1|i} = F_{i+1} \boldsymbol{\mu}_i = F_{i+1} \hat{\mathbf{x}}_i \\
&P_{i+1|i} = \mathbb{V}\mathbf{X}_{i+1|i} = F_{i+1} \boldsymbol{\Sigma}_i F_{i+1}^\top + Q_{i+1} = F_{i+1} P_i F_{i+1}^\top + Q_{i+1}
\end{align}

posterior distribution
\begin{align}
&\hat{\mathbf{x}}_{i+1} = \mathbb{E} \mathbf{X}_{i+1} = \boldsymbol{\mu}_{i+1|i} + K_{i+1} (\mathbf{z}_{i+1} - H_{i+1} \boldsymbol{\mu}_{i+1|i}) = \hat{\mathbf{x}}_{i+1|i} + K_{i+1} (\mathbf{z}_{i+1} - H_{i+1} \hat{\mathbf{x}}_{i+1|i} ) \\
&P_{i+1} = \mathbb{V}\mathbf{X}_{i+1} = (I - K_{i+1} H_{i+1}) \boldsymbol{\Sigma}_{i+1|i} = (I - K_{i+1} H_{i+1}) P_{i+1|i} \\ \notag \\
&\hspace{35pt} K_{i+1} = P_{i+1|i} H_{i+1}^\top (H_{i+1} P_{i+1|i} H_{i+1}^\top + R_{i+1})^{-1}
\end{align}

This can be neatly summarized in the table,
\begin{align}
&\hspace{80pt} \textbf{Prior} \hspace{45pt} \textbf{Intermediate} \hspace{110pt} \textbf{Posterior} \notag \\
&\textbf{Estimator:} \hspace{37pt} \hat{\mathbf{x}}_i \hspace{31pt} \hat{\mathbf{x}}_{i+1|i} = F_{i+1} \hat{\mathbf{x}}_i \hspace{70pt} \hat{\mathbf{x}}_{i+1} = \hat{\mathbf{x}}_{i+1|i} + K_{i+1} (\mathbf{z}_{i+1} - H_{i+1} \hat{\mathbf{x}}_{i+1|i} ) \notag \\
&\textbf{Uncertainty:} \hspace{27pt} P_i \hspace{30pt} P_{i+1|i} = F_{i+1} P_i F_{i+1}^\top + Q_{i+1} \hspace{17pt} P_{i+1} = (I - K_{i+1} H_{i+1}) P_{i+1|i} \notag
\end{align}

Notice the role of the gain matrix, defined above in (\ref{eq:gain_matrix}): the updated estimate for the hidden state vector is a weighted linear combination of the prior value, and the difference between measured and expected values, while the uncertainty that attends the update is progressively reduced as additional measurements are made.  Both the weighting factors for combination and the reduction of uncertainty are controlled by the values of the gain matrix.

\subsection{Linear Model as a Kalman Filter}
We can recover the solution of the linear model, presented above in $\S$\ref{sec:linear_models} and in \textit{PAA}, $\S$3.2, from the Kalman filter by introducing a trivial evolution equation, defined by
\begin{align}
F_{i+1} = I; \\
Q_{i+1} = 0.
\end{align} 

Furthermore, by changing names of the measurement variables,
\begin{align}
H_{i+1} &\rightarrow \mathbf{a}_{i+1}^\top \\
R_{i+1} &\rightarrow 1 \\
z_{i+1} &\rightarrow b_{i+1}
\end{align}
and the covariance and gain matrices,
\begin{align}
&P_i \hspace{44pt}= (A_i^\top A_i)^{-1} \\
&K_{i+1} \equiv \mathbf{k}_{i+1} =  \frac{(A_i^\top A_i)^{-1} \mathbf{a}_{i+1}}{\mathbf{a}_{i+1}^\top (A_i^\top A_i)^{-1} \mathbf{a}_{i+1} + 1}
\end{align}
we find the iterative solution to the linear model as
\begin{align}
&\textbf{Estimator:} \hspace{39pt}  \hat{\mathbf{x}}_{i+1} = \left( I - \mathbf{k}_{i+1} \mathbf{a}_{i+1}^\top \right) \hat{\mathbf{x}}_i + \mathbf{k}_{i+1} b_{i+1} \\
&\textbf{Uncertainty:} \hspace{27pt} \left(A_{i+1}^\top A_{i+1} \right)^{-1} = \left( I - \mathbf{k}_{i+1} \mathbf{a}_{i+1}^\top \right) \left(A_i^\top A_i \right)^{-1} 
\end{align}

\section{Generalized Linear Models}\label{sec:generalized_linear_models}
The linear model links predictor and response variables whose joint and marginal distributions are Gaussian in form.



simple exponential family adapted to distributions with location and scale parameters

\begin{equation}
\mathbf{Y} \Rightarrow p(\mathbf{y} | \boldsymbol{\theta} ) = h(\mathbf{y}) \exp (\eta(\boldsymbol{\theta})^\top T(\mathbf{y}) - A(\eta(\boldsymbol{\theta})) \propto \exp \left( \frac{\mathbf{y}^\top \boldsymbol{\theta}(\boldsymbol{\mu}) - b(\boldsymbol{\theta}(\boldsymbol{\mu}))}{\phi} \right)
\end{equation}

\begin{equation}
\boldsymbol{\theta} = \begin{pmatrix} \boldsymbol{\theta}(\boldsymbol{\mu}) \\ \phi \end{pmatrix}
\end{equation}

\begin{align}
&\mathbb{E} \mathbf{y} \equiv \boldsymbol{\mu} = \nabla_{\boldsymbol{\theta}} \, b(\boldsymbol{\theta}) \\
&\mathbb{V} \mathbf{y} \equiv \Sigma = \phi \nabla^2_{\boldsymbol{\theta}} \, b(\boldsymbol{\theta})
\end{align}


\begin{equation}
\mathbf{a}^\top\boldsymbol{\alpha} \equiv \eta
\end{equation}


link function
\begin{equation}
\boldsymbol{\mu}(\boldsymbol{\theta}) = \mathbf{h}(\eta)
\end{equation}

natural link function
\begin{equation}
\begin{rcases}
\boldsymbol{\mu}(\boldsymbol{\theta}) = \boldsymbol{\theta} \\
\mathbf{h}(\eta) = \eta
\end{rcases} \Rightarrow
\boldsymbol{\theta} = \eta = \mathbf{a}^\top\boldsymbol{\alpha}
\end{equation}


Generalized Linear Model:
\begin{equation}
\mathbb{E}\mathbf{y} = \boldsymbol{\mu} = h(\mathbf{a}^\top \boldsymbol{\alpha})
\end{equation}


IID 
\begin{equation}
\mathbf{y}_i \propto\exp \left( \frac{\mathbf{y}^\top \boldsymbol{\theta}(\boldsymbol{\mu}) - b(\boldsymbol{\theta}(\boldsymbol{\mu}))}{\phi} \right) \Rightarrow \prod_{i = 1}^N 
\end{equation}



\section{Time Series}\label{sec:time_series}
A \textbf{time series} is a list of random variables, $X_0, \cdots, X_n$, indexed by time, with the initial sample taken at time, $t_0$, and subsequent values taken at equal time increments, $\Delta t$,
\begin{align}
&X_0 = X(t_0), \\
&X_i = X(t_0 + i \Delta t).
\end{align}
The various methods of time series analysis assert
\begin{itemize}[noitemsep]
\item models for the uncertainty of random variables;
\item relationships among random variables and uncertainties.
\end{itemize}
If the value of a random variable depends only on prior random variables, the time series is \textbf{causal}; if the distribution of the limiting random variable satisfies any finite bound, the time series is \textbf{non-divergent}; if the statistical properties of random variables, including means, variances and covariances, are constant in time, the time series is \textbf{stationary}.  

A \textbf{measurement} is a realized value of a random variable, while an \textbf{innovation} is the difference between the realized and estimated values.  Causal time series link measurements and innovations at one point in time to measurements and innovations at prior points.

Time series are examples of \textbf{discrete stochastic processes}.

\subsection{ARIMA Process}\label{sec:ARIMA_process}
If, given a causal time series, $X_0, \cdots, X_i, \cdots$, the model for uncertainty is constant and the relationship between any given random variable and prior random variables is fixed and linear, then we have an ARIMA process:

\begin{itemize}[noitemsep]
\item AR: \hspace{2pt} Autoregressive \hspace{11pt} random variables are dependent on prior \textit{measurements};
\item I: \hspace{14pt} Integrated \hspace{28pt} neighboring random variables are subject to differencing operations;
\item MA: \hspace{1pt} Moving Average \hspace{3pt} random variables are dependent on prior \textit{innovations}.
\end{itemize}

The number of prior measurements or innovations and the number of differencing operations are referred to as the \textbf{orders} of the process.  A full ARIMA process is expressed as

\begin{equation}
\operatorname{ARIMA}(p, d, q) \Rightarrow
\begin{cases}
\operatorname{AR}(p) \hspace{5pt} \text{Random variables depend on \textit{p} prior measurements} \\
\operatorname{I}(d) \hspace{16pt} \text{Random variables are subject to \textit{d} differencing operations} \\
\operatorname{MA}(q) \hspace{4pt} \text{Random variables depend on \textit{q} prior innovations}
\end{cases}
\end{equation}

Most of the efforts here are directed towards AR processes, especially considering conditions under which the time series is stationary.  We shall see that 

\begin{itemize} [noitemsep]
\item Non-stationary time series may be made stationary through differencing (integration);
\item Autoregressive time series of finite order are equivalent to moving average time series of infinite order (and \textit{v.v.}).
\end{itemize}

\subsubsection{AR(\textit{p}) Process}\label{sec:ARp_process}
Given a time series of random variables, $X_0, \cdots, X_n$, an AR$(p)$ model links each random variable to the $p$ prior random variables by fixed linear combination, with each measurement subject to constant uncertainty,
\begin{equation}\label{eq:ARp}
X_t = \sum_{j=1}^p \phi_j X_{t-j} + \epsilon_t \Rightarrow
\begin{cases}
\mathbb{E} \epsilon = 0 \\
\mathbb{V} \epsilon = 1
\end{cases} p \leq t \leq n.
\end{equation}

The equation in (\ref{eq:ARp}) can be more neatly expressed in terms of the backshift operator, $\mathcal{B}$, which decrements the index of a random variable,
\begin{equation}
\mathcal{B} X_t \equiv X_{t-1} \Rightarrow \left(1 - \sum_{j=1}^p \phi_j \mathcal{B}^j \right) X_t = \epsilon_t .
\end{equation}

\paragraph{Formal Solution to an AR(\textit{p}) Process}\label{sec:ARp_formal_solution} $\\ \\$
The backshift operator is linear, associative and distributive with respect to scalar multiplication -- power expressions in back shift operators can therefore be factored and otherwise treated as polynomials.  Indeed, we can factor the $p^{\text{th}}$-order backshift polynomial equation into $p$ first-order terms, each governed by a root, $\lambda_j$, so that
\begin{equation}\label{eq:ARp_polynomial}
1 - \sum_{j=1}^p \phi_j \mathcal{B}^j = \prod_{j=1}^p \left( 1 - \lambda_j \mathcal{B} \right).
\end{equation}
Upon application to random variables, the operators on both sides of the equation in (\ref{eq:ARp_polynomial}) yield identical results.

Using the method of partial fractions, the inverse polynomial can be expanded in poles of the roots, so that
\begin{equation}\label{eq:ARp_poles}
\left( 1 - \sum_{j=1}^p \phi_j \mathcal{B}^j \right)^{-1} = \sum_{j=1}^p \gamma_j \left( 1 - \lambda_j \mathcal{B} \right)^{-1}, \hspace{10pt}
\gamma_j = \prod_{i\neq j}\frac{1}{\lambda_i - \lambda_j}.
\end{equation}

The magnitude of the backshift operator is defined by its effects on random variables upon which it operates.  Since the backshift operator changes only the index of the random variable with no effect on scale, the magnitude is unity.  If any given root of the polynomial equation in (\ref{eq:ARp_polynomial}) has magnitude less than unity, then the product of root and backshift operator is less than unity as well,
\begin{equation}
\begin{rcases}
|\mathcal{B}| = 1 \\
|\lambda| < 1
\end{rcases} \Rightarrow
|\lambda \mathcal{B} | < 1.
\end{equation}

Finally, provided the magnitude of the root is less than unity, we can expand the poles in (\ref{eq:ARp_poles}) into an infinite geometric series,
\begin{equation}\label{eq:ARp_pole_expansion}
(1 - \lambda \mathcal{B})^{-1} = \sum_{k=0}^\infty \lambda^k \mathcal{B}^k.
\end{equation}
Here, the equality in (\ref{eq:ARp_pole_expansion}) follows from expanding the direct calculation,
\begin{equation}
\left(1 - \lambda \mathcal{B} \right) \sum_{k=0}^\infty \lambda^k \mathcal{B}^k = 1 + \sum_{k=1}^{\infty} \lambda^k \mathcal{B}^k - \lambda \mathcal{B} \sum_{k=0}^\infty \lambda^k \mathcal{B}^k = 1,
\end{equation}
which holds provided the effects of operating on random variables encoded in the infinite sums converge.  Convergence therefore requires the magnitude of the root be less than unity.

We can express the inverse operator in terms of the sums and products of powers and partial sums of roots, 
\begin{equation}
\Gamma_k \equiv \Gamma_k(\lambda_1, \cdots, \lambda_p) = \sum_{j=1}^p \prod_{i\neq j}^p \frac{\lambda_j^k}{\lambda_i - \lambda_j},
\end{equation}
whose value is dominated by the largest root scaled by a constant.  For the case in which all roots have magnitude less than unity, the elements vanish, dominated by a power series, so that
\begin{equation}\label{eq:ARp_asymptotic_Gamma}
\lim_{k \to \infty} \Gamma_k = 0.
\end{equation}

Applying the algebraic properties defined above, we can express the random variable, $X_t$, succinctly as an infinite series of backshift operations on the innovation, $\epsilon_t$, provided the roots have magnitude all less than unity:
\begin{align}
  X_t &= \left(1 - \sum_{j=1}^p \phi_j \mathcal{B}^j \right)^{-1} \epsilon_t
= \left( \sum_{j=1}^p \gamma_j \left( 1 - \lambda_j \mathcal{B} \right)^{-1} \right)  \epsilon_t
= \left( \sum_{j=1}^p \gamma_j \sum_{k=0}^\infty \lambda_j^k \mathcal{B}^k \right) \epsilon_t \notag \\
&= \left( \sum_{j=1}^p \left( \prod_{i\neq j}^p \frac{1}{\lambda_i - \lambda_j} \right) \sum_{k=0}^\infty \lambda_j^k \mathcal{B}^k \right) \epsilon_t
= \left(  \sum_{k=0}^\infty  \left( \sum_{j=1}^p \prod_{i\neq j}^p \frac{\lambda_j^k}{\lambda_i - \lambda_j} \right)  \mathcal{B}^k \right) \epsilon_t \notag \\
&\equiv \left(  \sum_{k=0}^\infty \Gamma_k \mathcal{B}^k \right) \epsilon_t. \label{eq:ARp_formal_solution}
\end{align}
Every stable finite-order AR model is formally equivalent to an infinite-order MA model.

\paragraph{Explicit Asymptotic Solution using Cyclic Permutation Matrices} $\\ \\$
We can calculate explicit solutions to the AR$(p)$ process in (\ref{eq:ARp}) by identifying the shift operator, $\mathcal{B}$, applied to a time series of $n+1$ elements, with the $(n+1) \times (n+1)$ cyclic permutation matrix, $U_{n+1}$.  This matrix has unit values on the super diagonal and in the lower left-hand corner.  Repeated applications of the cyclic permutation matrix push the unit values onto progressively higher diagonals, terminating with the identity matrix:
\begin{equation}
\mathcal{B} \equiv U_{n+1} = 
\begin{pmatrix}
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1 \\
1 & 0 & \cdots & 0
\end{pmatrix},
U _{n+1}^2 =
\begin{pmatrix}
0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0
\end{pmatrix}, \cdots ,
U_{n+1}^n = I_{n+1}
\end{equation}

Properties of the shift operator are retained in the linear algebraic properties of cyclic permutation matrices:
\begin{itemize}
\item The inverse and transpose matrices coincide,
\begin{equation}\label{eq:CPM_inverse}
\left( U_{n+1}^k \right)^{-1} = \left( U_{n+1}^k \right)^\top \Rightarrow U_{n+1}^k \left( U_{n+1}^l \right)^\top = 
\begin{cases}
U_{n+1}^{k-l}, \hspace{10pt} k > l \\
I_{n+1}, \hspace{12pt} k = l \\
U_{n+1}^{l-k}, \hspace{10pt} k < l
\end{cases}
\end{equation}
\item All eigenvalues, which can be calculated directly, have unit magnitude in any matrix norm,
\begin{equation}\label{eq:CPM_eigenvalues}
\operatorname{eig} \left( U_{n+1} \right) = \left\{ \exp \left( \frac{2 \pi i}{n+1} j \right), j = 1, \cdots, n+1 \right\} \Rightarrow \left| \operatorname{eig} ( U_{n+1}) \right| = 1.
\end{equation}
\end{itemize}
The key `additional' properties of cyclic permutation matrices, which introduce properties at variance with shift operators, are observed in (\ref{eq:CPM_inverse}): repeated applications of the inverse operation cycle through prior states, since $U_{n+1}^{n+k} = U_{n+1}^k$.  This property is obviously non-causal; however, we show below that the error attributable to non-causal interactions is reduced as the number of observations, $n$, increases. In the limit of infinite observations the error vanishes, and causality is restored to the solution.

Interpreting the shift operator as a cyclic permutation matrix, an \textit{augmented} version of the equation in (\ref{eq:ARp}) is given by,
\begin{equation}\label{eq:ARp_shift_matrix}
\left( I_{n+1} - \sum_{j=1}^p \phi_j U_{n+1}^j \right) \boldsymbol{X} = \boldsymbol{\epsilon},
\end{equation}
for which the sequence of random variables and innovations are expressed as vectors.  Expanding the matrices into component form, the equation in (\ref{eq:ARp_shift_matrix}) yields a banded set of linear equations that, for any \textit{finite} $n$, has a unique solution.  Note that the expression in (\ref{eq:ARp}) covers the top $n-p+1$ equations; the remainder `wrap around' the interval and express the first $p$ random variables partly as functions of the last $p$ random variables. 
\begin{equation}
\begin{pmatrix}
1 & -\phi_1 & \cdots & -\phi_p & 0 & 0 & \cdots & 0 & 0 \\
0 & 1 & \cdots & -\phi_{p-1} & -\phi_p & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & 1 & -\phi_1 & -\phi_2 & \cdots & -\phi_p & 0 \\
0 & 0 & \cdots & 1 & 1 & -\phi_1 & \cdots & -\phi_{p-1} & -\phi_p \\
-\phi_p & 0 & \cdots & 0 & 0 & 1 & \cdots & -\phi_{p-2} & -\phi_{p-1} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
-\phi_2 & -\phi_3 & \cdots & -\phi_p & 0 & 0 & \cdots & 1 & -\phi_1 \\
-\phi_1 & -\phi_2 & \cdots & -\phi_{p-1} & \phi_p & 0 & \cdots & 0 & 1
\end{pmatrix}
\begin{pmatrix}
X_n \\
X_{n-1} \\
\vdots \\
X_{p+1} \\
X_p \\
X_{p-1} \\
\vdots \\
X_1 \\
X_0
\end{pmatrix} =
\begin{pmatrix}
\epsilon_n \\
\epsilon_{n-1} \\
\vdots \\
\epsilon_{p+1} \\
\epsilon_p \\
\epsilon_{p-1} \\
\vdots \\
\epsilon_1 \\
\epsilon_0
\end{pmatrix}
\end{equation}
 This is, of course, non-causal, but we shall show that the non-causal contribution to the solution decreases with increasing history, and vanishes completely in the asymptotic limit  of infinite measurements.

The link between expressing the solution to the AR$(p)$ process in terms of shift operators and cyclic permutation matrices is made by identifying both the identity and shift operator in (\ref{eq:ARp_formal_solution}) with the appropriate matrices,
\begin{align}
\begin{rcases}
1 \rightarrow I_{n+1} \\
\mathcal{B} \rightarrow U_{n+1}
\end{rcases} 
&\Rightarrow \left( 1 - \sum_{j=1}^p \phi_j \mathcal{B}^j \right) \rightarrow \left( I_{n+1} - \sum_{j=1}^p \phi_j U_{n+1}^j \right) \notag \\
&\Rightarrow \sum_{j=1}^p \gamma_j \left( 1 - \lambda_j \mathcal{B} \right)^{-1} \rightarrow \sum_{j=1}^p \gamma_j \left( I_{n+1} - \lambda_j U_{n+1} \right)^{-1} \notag \\
&\Rightarrow \sum_{k=0}^\infty \Gamma_k \mathcal{B}^k \rightarrow \sum_{k=0}^\infty \Gamma_k U_{n+1}^k,
\end{align}
which shows that, for the case in which all roots have magnitude less than unity, $\lambda_j < 1$, the solution takes the form,
\begin{equation}\label{eq:ARp_solution}
\mathbf{X} = \left( \sum_{k=0}^\infty \Gamma_k U_{n+1}^k \right) \boldsymbol{\epsilon}.
\end{equation}

For finite measurements, $n < \infty$, the solution can be further factored, since by the cyclic nature of the matrices,
\begin{equation}
\mathbf{X} = \left( \sum_{k=0}^n \left( \sum_{l=0}^\infty \Gamma_{kl} \right) U_{n+1}^k \right) \boldsymbol{\epsilon}.
\end{equation}

Since by the relation in (\ref{eq:ARp_asymptotic_Gamma}) the higher-order constants, dominated by a power series, vanish in the limit and we have
\begin{equation}
\lim_{n \to \infty} \sum_{k=0}^n \left( \sum_{l=0}^\infty \Gamma_{kl} \right) = \Gamma_k,
\end{equation}
which implies that the non-causal contribution to the solution to the AR$(p)$ process based on cyclic permutation matrices in (\ref{eq:ARp_solution}) vanishes in the limit of infinite history.

\paragraph{Lagged Covariance Structure of the Formal Solution}\label{sec:ARp_covariance_structure} $\\ \\$
The lagged autocovariance matrix of the solution in (\ref{eq:ARp_solution}) can be constructed directly and, since the lagged autocovariance matrix for innovations is the identity, $\mathbb{E}\boldsymbol{\epsilon}\boldsymbol{\epsilon}^\top = I_{n+1}$, takes the form,
\begin{align}
\mathbb{E} \mathbf{X} \mathbf{X}^\top &= \mathbb{E} \left( \left( \sum_{k=0}^\infty \Gamma_k U_{n+1}^k \right) \boldsymbol{\epsilon} \right) \left(  \left( \sum_{l=0}^\infty \Gamma_l U_{n+1}^l \right) \boldsymbol{\epsilon} \right)^\top \\
&= \left( \sum_{k=0}^\infty \Gamma_k U_{n+1}^k \right) \mathbb{E} \boldsymbol{\epsilon} \boldsymbol{\epsilon}^\top \left( \sum_{l=0}^\infty \Gamma_l U_{n+1}^l \right)^\top \\
&= \left( \sum_{k=0}^\infty \Gamma_k U_{n+1}^k \right) \left( \sum_{l=0}^\infty \Gamma_l U_{n+1}^l \right)^\top .
\end{align}

Since the inverse and transpose matrices coincide for the cyclic permutation matrix, and nonzero entries in the summed matrices do not overlap, the entries in the asymptotic result as $n$ diverges take the form,
\begin{equation}
\lim_{n\rightarrow\infty} \mathbb{E} \mathbf{X} \mathbf{X}^\top \equiv \boldsymbol{\Sigma} \Rightarrow
\boldsymbol{\Sigma}_{ij} = \sum_{k=0}^\infty \Gamma_k \Gamma_{k + |i - j|}.
\end{equation}
Furthermore, as the elements of the matrix depend only on the relative difference between $i$ and $j$, the solution is stationary for constant coefficients, $\phi_1, \cdots, \phi_p$, provided the magnitudes of the roots, $\lambda_1, \cdots, \lambda_p$, are less than unity.


\paragraph{Parameter Estimation (Yule-Walker Equations)}\label{sec:ARp_parameter_estimation} $\\ \\$
The presentation in $\S\S$\ref{sec:ARp_formal_solution} \& \ref{sec:ARp_covariance_structure} provides conditions under which the solution to the AR$(p)$ processis stationary.  Here, we take the complementary problem: given a set of data points, $x_0, \cdots, x_n$, estimate the parameters, $\phi_0, \cdots, \phi_p$, of the AR$(p)$ process that best fits the data.

Taking the expectation of both sides of the equation in (\ref{eq:ARp}), we can express the AR$(p)$ process as a linear combination of lagged autocovariances,
\begin{equation}
\mathbb{E} X_t X_{t-k} = \sum_{j-1}^p \phi_j \mathbb{E} X_t X_{t-k} .
\end{equation}
Furthermore, assuming that the process is stationary, 
\begin{equation}
\mathbb{E} X_t X_{t-k} = \mathbb{E} X_0 X_k
\end{equation}
we can form a linear equation -- the  \textbf{Yule-Walker equation} -- whose coefficients are lagged autocovariances,

\begin{equation}\label{eq:Yule_Walker}
\begin{pmatrix}
\mathbb{E} X_0 X_0 & \mathbb{E} X_0 X_1 & \cdots & \mathbb{E} X_0 X_{p-1} \\
\mathbb{E} X_0 X_1 & \mathbb{E} X_0 X_0 & \cdots & \mathbb{E} X_0 X_{p-2} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbb{E} X_0 X_{p-1} & \mathbb{E} X_0 X_{p-2} & \cdots & \mathbb{E} X_0 X_0
\end{pmatrix}
\begin{pmatrix}
\phi_1 \\
\phi_2 \\
\vdots \\
\phi_p
\end{pmatrix} = 
\begin{pmatrix}
\mathbb{E} X_0 X_1 \\
\mathbb{E} X_0 X_2 \\
\vdots \\
\mathbb{E} X_0 X_p.
\end{pmatrix}
\end{equation}

Given the empirical lagged autocovariances,
\begin{equation}
\hat{\sigma}_{0k} = \frac{1}{n-k} \sum_{i=1}^{n-k} x_i x_{i+k}
\end{equation}
the Yule-Walker equation can be inverted to yield the coefficients,
\begin{equation}
\begin{rcases}
\hat{\boldsymbol{\Sigma}} =
\begin{pmatrix}
\hat{\sigma}_{00} & \hat{\sigma}_{01} & \cdots & \hat{\sigma}_{0,p-1} \\
\hat{\sigma}_{01} & \hat{\sigma}_{00} & \cdots & \hat{\sigma}_{0,p-2} \\
\vdots & \vdots & \ddots & \vdots \\
\hat{\sigma}_{0,p-1} & \hat{\sigma}_{0,p-2} & \cdots & \hat{\sigma}_{00}
\end{pmatrix} \\
\hat{\mathbf{v}} =
\begin{pmatrix}
\hat{\sigma}_{01} \\
\hat{\sigma}_{02} \\
\vdots \\
\hat{\sigma}_{0p}
\end{pmatrix}
\end{rcases}
\Rightarrow
\boldsymbol{\phi} = \hat{\boldsymbol{\Sigma}}^{-1} \hat{\mathbf{v}}.
\end{equation}.

\subsubsection{MA(\textit{q}) Process}

\subsubsection{Unit Roots (I(\textit{d}) Process)}
The formal expression of the AR$(p)$ model in terms of a polynomial expansion of the backshift operator, provided in (\ref{eq:ARp_polynomial}), is given by 
\begin{equation}
\left(1 - \sum_{j=1}^p \phi_j \mathcal{B}^j \right) X_t = \left( \prod_{j=1}^p \left( 1 - \lambda_j \mathcal{B} \right) \right) X_t = \epsilon_t.
\end{equation}
As shown above, a solution to this equation exists provided that the magnitude of all roots be strictly less than unity, $|\lambda_j| < 1, 1 \leq j \leq p$.

If, however, there is a unit root of multiplicity, $d$, such that the factored polynomial can be rearrangeed as,
\begin{equation}
 \prod_{j=1}^p \left( 1 - \lambda_j \mathcal{B} \right) \equiv  \left( \prod_{j=1}^{p - d} \left( 1 - \lambda_j \mathcal{B} \right) \right) \left( 1 - \mathcal{B} \right)^d,
\end{equation}
for which the root indices are labeled so that the final $d$ have unit value.  Then the AR$(p)$ process can be refactored as
\begin{equation}\label{ARp_differencing}
 \left( \prod_{j=1}^p \left( 1 - \lambda_j \mathcal{B} \right) \right) X_t = \left( \prod_{j=1}^{p - d} \left( 1 - \lambda_j \mathcal{B} \right) \right) ( 1 - \mathcal{B} )^d X_t = \left( \prod_{j=1}^{p - d} \left( 1 - \lambda_j \mathcal{B} \right) \right) X^{'}_t
\end{equation}
for which the unit root is incorporated directly into the random variable as a $d^{th}$-order differenced variable, $X^{'}_t =  ( 1 - \mathcal{B} )^d X_t$.  Here, the transformed time series is stable, since the magnitude of all remaining roots are less than unity.

 The absorption of the unit roots in (\ref{ARp_differencing}) is equivalent to applying a first-order finite-difference scheme to the time series that approximates the $d^{th}$ derivative operator.


\paragraph {Dickey-Fuller Test}


\end{document}